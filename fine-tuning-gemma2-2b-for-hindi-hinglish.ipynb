{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU",
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 85416,
          "databundleVersionId": 9690815,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30822,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nileshchopda/Fine-Tuning_Gemma2_with_LoRA_for_Hindi/blob/main/fine-tuning-gemma2-2b-for-hindi-hinglish.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine Tuning Gemma2 2b for Hindi-Hinglish QnA"
      ],
      "metadata": {
        "id": "VFsfZOgZ30Xs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook we will walk through our step by step journey of fine tuning **Gemma2 2b model** into a conversational chatbot for Hindi.\n",
        "\n",
        "This notebook will help you\n",
        "  \n",
        "  1. Understand the various parameters of crucial LoRA Layers. Where you will learn what the parameters do and how to tailor them for knowledge injection vs task tuning.\n",
        "\n",
        "  2. How to effectively save your models after training and avoid unintentional inference issues.\n",
        "\n",
        "  3. How to save resources while training as much as possible.\n",
        "\n",
        "We used following datasets for our fine tuning.\n",
        "- **GPT4's Alpaca**\n",
        "- **Cognitive Lab's Hindi Instruct**\n",
        "- **Wikipedia Datasets**\n",
        "- **Alpaca for Gemma**\n",
        "- **Databricks Dolly**\n",
        "- **Hindi Maths Quest**\n",
        "\n",
        "We experimented with different LoRA Configs in the various trainings with varying results depending on the configs\n",
        "\n",
        "We mostly used **L4** and occasionaly **A100** GPUs as per our resource availability.\n",
        "\n",
        "It took us about **40 Hours** to train on all the datasets, either full or a subset of them.\n",
        "\n",
        "We learnt many things along the way specially related to the prompt for fine tuning and suitable LoRA Config for different tasks.\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**All of our models and adapters that has been used/created in this project are available on kaggle**\n",
        "\n",
        "\n",
        "You will need to place your kaggle token json file inside /root/.config/kaggle folder, you can download it from kaggle.\n",
        "\n",
        "```python\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.model_download(\"lnshrivas/gemma-2/transformers/gemma-2-2b-hindi\")\n",
        "\n",
        "print(\"Path to model files:\", path)\n",
        "```\n",
        "\n",
        "**Please pay attention to the `Notes` sections they are important to understand some crucial steps and concepts**\n",
        "\n",
        "**Please check the `Understanding LoRA` section inside the *Conclusion* section before proceeding for enhancing your understanding about training from the getgo**\n",
        "\n",
        "---\n",
        "\n",
        ">DISCLAIMER - This Notebook was created as part of a Kaggle Competition"
      ],
      "metadata": {
        "id": "8OQRHEUp3929"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Inference Testing on base model"
      ],
      "metadata": {
        "id": "IMPUVl3b61Jj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let us test our base model's capacity for handling English, Hindi and Hinglish queries."
      ],
      "metadata": {
        "id": "755_rE-J-M4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install datasets\n",
        "!pip install trl\n",
        "!pip install kaggle\n",
        "!pip install peft"
      ],
      "metadata": {
        "id": "71K_hWbqQjad",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T09:03:15.981578Z",
          "iopub.execute_input": "2025-02-28T09:03:15.981929Z",
          "iopub.status.idle": "2025-02-28T09:03:33.262654Z",
          "shell.execute_reply.started": "2025-02-28T09:03:15.981896Z",
          "shell.execute_reply": "2025-02-28T09:03:33.261512Z"
        },
        "outputId": "cca5caa4-24e6-4734-9a14-d82effde675a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.5.1+cpu)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.3-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.3\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.3.2-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting aiohttp (from datasets)\n",
            "  Downloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
            "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
            "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
            "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
            "  Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
            "  Downloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
            "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
            "  Downloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (69 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.2/69.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.2-py3-none-any.whl (485 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m485.4/485.4 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp-3.11.13-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
            "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading frozenlist-1.5.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (274 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.9/274.9 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading propcache-0.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.3/231.3 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yarl-1.18.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (344 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m344.1/344.1 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, propcache, fsspec, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.2.0\n",
            "    Uninstalling fsspec-2025.2.0:\n",
            "      Successfully uninstalled fsspec-2025.2.0\n",
            "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.13 aiosignal-1.3.2 datasets-3.3.2 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multiprocess-0.70.16 propcache-0.3.0 xxhash-3.5.0 yarl-1.18.3\n",
            "Collecting trl\n",
            "  Downloading trl-0.15.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: accelerate>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from trl) (1.3.0)\n",
            "Requirement already satisfied: datasets>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from trl) (3.3.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from trl) (13.9.4)\n",
            "Requirement already satisfied: transformers>=4.46.0 in /usr/local/lib/python3.11/dist-packages (from trl) (4.48.3)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (2.5.1+cpu)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.34.0->trl) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.17.0)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (19.0.1)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets>=2.21.0->trl) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.21.0->trl) (3.11.13)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.46.0->trl) (0.21.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->trl) (2.19.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.21.0->trl) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate>=0.34.0->trl) (4.12.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->trl) (0.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets>=2.21.0->trl) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate>=0.34.0->trl) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate>=0.34.0->trl) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=2.21.0->trl) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.21.0->trl) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate>=0.34.0->trl) (3.0.2)\n",
            "Downloading trl-0.15.2-py3-none-any.whl (318 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: trl\n",
            "Successfully installed trl-0.15.2\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n",
            "Collecting peft\n",
            "  Downloading peft-0.14.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.5.1+cpu)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.48.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.3.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.28.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2024.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.5)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n",
            "Downloading peft-0.14.0-py3-none-any.whl (374 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.8/374.8 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: peft\n",
            "Successfully installed peft-0.14.0\n"
          ]
        }
      ],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import warnings\n",
        "\n",
        "from trl import SFTTrainer\n",
        "from peft import PeftModel\n",
        "from peft import LoraConfig\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "\n",
        "from transformers import BitsAndBytesConfig, TrainingArguments, AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq"
      ],
      "metadata": {
        "tags": [],
        "id": "dAY3N3lztFfwaZfNoakSGNmn",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T09:03:39.792547Z",
          "iopub.execute_input": "2025-02-28T09:03:39.792851Z",
          "iopub.status.idle": "2025-02-28T09:03:39.885260Z",
          "shell.execute_reply.started": "2025-02-28T09:03:39.792829Z",
          "shell.execute_reply": "2025-02-28T09:03:39.884621Z"
        },
        "outputId": "b4fd20bc-ee4c-49b5-aa19-e2fa787feba1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch_xla/__init__.py:253: UserWarning: `tensorflow` can conflict with `torch-xla`. Prefer `tensorflow-cpu` when using PyTorch/XLA. To silence this warning, `pip uninstall -y tensorflow && pip install tensorflow-cpu`. If you are in a notebook environment such as Colab or Kaggle, restart your notebook runtime afterwards.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "execution_count": 2
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will download our base model first, from Kaggle.\n",
        "\n",
        "You will also need a consent from google on kaggle to get this model. It's quite easy\n",
        "\n",
        "https://www.kaggle.com/models/google/gemma-2/transformers/gemma-2-2b"
      ],
      "metadata": {
        "id": "LnK38nVvwzDL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install kaggle"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T09:05:18.575962Z",
          "iopub.execute_input": "2025-02-28T09:05:18.576300Z",
          "iopub.status.idle": "2025-02-28T09:05:18.580195Z",
          "shell.execute_reply.started": "2025-02-28T09:05:18.576273Z",
          "shell.execute_reply": "2025-02-28T09:05:18.579242Z"
        },
        "id": "djWq2UAh2uF5",
        "outputId": "4288c408-bf94-40e9-adaf-4f04884d56f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.3.0)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.11/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kaggle) (3.10)\n"
          ]
        }
      ],
      "execution_count": 3
    },
    {
      "cell_type": "code",
      "source": [
        "# from kaggle_secrets import UserSecretsClient\n",
        "# user_secrets = UserSecretsClient()\n",
        "# secret_value_0 = user_secrets.get_secret(\"key\")\n",
        "# secret_value_1 = user_secrets.get_secret(\"username\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T09:10:47.368480Z",
          "iopub.execute_input": "2025-02-28T09:10:47.368855Z",
          "iopub.status.idle": "2025-02-28T09:10:47.966785Z",
          "shell.execute_reply.started": "2025-02-28T09:10:47.368829Z",
          "shell.execute_reply": "2025-02-28T09:10:47.966060Z"
        },
        "id": "FLHXOdgn2uF6"
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "code",
      "source": [
        "# !ls ..\n",
        "# !mkdir ../root/.config/kaggle\n",
        "## Get kaggle.json form kaggle with api\n",
        "# !cp kaggle.json /root/.config/kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "e0LbIE0x39YH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle models instances versions download google/gemma-2/transformers/gemma-2-2b/2"
      ],
      "metadata": {
        "id": "GQ1sPsPtcY_V",
        "outputId": "b9a991d4-eb80-43c8-82e8-050fe20e256c",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T09:10:55.796208Z",
          "iopub.execute_input": "2025-02-28T09:10:55.796527Z",
          "iopub.status.idle": "2025-02-28T09:10:56.248369Z",
          "shell.execute_reply.started": "2025-02-28T09:10:55.796503Z",
          "shell.execute_reply": "2025-02-28T09:10:56.247281Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.config/kaggle/kaggle.json'\n",
            "Downloading gemma-2.tar.gz to /content\n",
            "100% 9.07G/9.07G [01:12<00:00, 144MB/s]\n",
            "100% 9.07G/9.07G [01:12<00:00, 134MB/s]\n",
            "/content/gemma-2.tar.gz"
          ]
        }
      ],
      "execution_count": 14
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll extract the model files"
      ],
      "metadata": {
        "id": "6r7uIxKNwjpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf 'gemma-2.tar.gz' 'gemma-2-2b'"
      ],
      "metadata": {
        "id": "OOx4f6nrxGW8",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-28T09:02:55.871820Z",
          "iopub.status.idle": "2025-02-28T09:02:55.872195Z",
          "shell.execute_reply": "2025-02-28T09:02:55.872019Z"
        }
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        ">NOTE: We will load the model in 4-bit quantization for resource constraint training"
      ],
      "metadata": {
        "id": "4IrglmBlTNjQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "IvUex4Zg8qCw",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gemma-2-2b\")"
      ],
      "metadata": {
        "id": "bV-kuq0T8uwv",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"gemma-2-2b\",quantization_config=bnb_config,\n",
        "                                                                         device_map='auto')"
      ],
      "metadata": {
        "id": "ox6BJVxp8uwv",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"<start_of_turn>user Tell me about elephants, but tell me in English please. <end of turn>\\n<start_of_turn>model \"\n",
        "\n",
        "inputs = tokenizer(question, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=246,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1,\n",
        "                              use_cache=False)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
      ],
      "metadata": {
        "id": "3ncd3olv81Zs",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"<start_of_turn>user recycling ke vishay me ek nara sujhav kare<end of turn>\\n<start_of_turn>model \"\n",
        "\n",
        "inputs = tokenizer(question, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=246,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1,\n",
        "                              use_cache=False)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
      ],
      "metadata": {
        "id": "G6HozyMK86XC",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"<start_of_turn>user रीसाइक्लिंग के विषय में एक नारा सुझाए<end of turn>\\n<start_of_turn>model \"\n",
        "\n",
        "inputs = tokenizer(question, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=246,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1,\n",
        "                              use_cache=False)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
      ],
      "metadata": {
        "id": "ed-8jR729uVb",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see it cannot handle hinglish nor hindi queries, Generating garbled output.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vZAaALf5_DWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Fine tuning on Alpaca Dataset and making it understand Hindi"
      ],
      "metadata": {
        "id": "k3vLclOR_OPg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We trained for a total of **15 Hrs** on this dataset"
      ],
      "metadata": {
        "id": "WJ7Cq6rMnWrG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Preparation\n",
        "\n",
        "> NOTE: These settings often help in reducing reserved GPU memory to increase available memory and can help in save resources for training."
      ],
      "metadata": {
        "id": "b_XUxI-CAPeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!export TORCH_CUDA_ALLOC_CONF=max_split_size_mb:128"
      ],
      "metadata": {
        "id": "bMxlHuXJ-AaV",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True"
      ],
      "metadata": {
        "id": "UGxOe6yQ8G0p",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_dataset_train = load_dataset(\"FreedomIntelligence/alpaca-gpt4-hindi\",\n",
        "                              split = \"train\")\n",
        "alpaca_dataset_train, alpaca_dataset_train[3]"
      ],
      "metadata": {
        "id": "HslX03Ey0L5-",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_dataset_train.info"
      ],
      "metadata": {
        "id": "vxI-dfgw_Htq",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_prompt=\"\"\"<start_of_turn>user\\n.\\n\\\"{}\\\"<end_of_turn>\\n<start_of_turn>model\\n{}<end_of_turn>\"\"\"\n",
        "print(alpaca_prompt)"
      ],
      "metadata": {
        "id": "bZPRizLp0pMn",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> NOTE: We have set the padding to right. This will ensure the padding will be added to the right of the tokens. Without this tokenizer may pad to the left."
      ],
      "metadata": {
        "id": "8RYuydULCFLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "eos_token = tokenizer.eos_token\n",
        "tokenizer.padding_side = \"right\"\n",
        "eos_token"
      ],
      "metadata": {
        "id": "0PAD_vZTiTlV",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(conversations):\n",
        "    texts = []\n",
        "    conversations = conversations[\"conversations\"]\n",
        "    for convo in conversations:\n",
        "        # EOS_TOKEN is important\n",
        "        text = alpaca_prompt.format(convo[0][\"value\"], convo[1][\"value\"]) + eos_token\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }"
      ],
      "metadata": {
        "id": "JL5NtRlB2Td9",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_dataset = alpaca_dataset_train.map(formatting_func, batched = True,)"
      ],
      "metadata": {
        "id": "Hu7-X4etrSf2",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_dataset"
      ],
      "metadata": {
        "id": "qamEJyXZ2Vb1",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> NOTE: This prompt will help our model understand the user query and model generation part. The eos (end of sentence) token is very important without which the model will use to know when to stop regardless of max seq length to generate otherwise it will generate endless tokens which we will see happening in action, ahead"
      ],
      "metadata": {
        "id": "j89eBukkAmXT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(alpaca_dataset[\"text\"][0])"
      ],
      "metadata": {
        "id": "6fgKorfNRB24",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> NOTE: This function converts the generated prompt text into tokens along with padding, and also generates attention masks for our tokens, telling the model which tokens to pay attention to and ignore the padding tokens"
      ],
      "metadata": {
        "id": "wqLQeEC4CZId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "dataset = alpaca_dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "print(\"Dataset tokenized:\", dataset[0])"
      ],
      "metadata": {
        "id": "nhe6eNy2Ql5U",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Training\n",
        "\n",
        "> NOTE: This LoRA Config helped us generalize faster with our dataset, however if you wish to inject knowledge to ur model\n",
        "- keep **\"r\"** high\n",
        "- remove the feed forward layers from target modules\n",
        "- use_rslora=False\n",
        "\n",
        "> However in our implementation we didn't follow through it.\n",
        "\n",
        "> To save resources while training\n",
        "- use smaller batch size\n",
        "- keep gradient accumulation to 1\n",
        "- save limit to 1 or 2\n",
        "- set a torch empty cache parameter other wise after long runs the build up of cache will crash the GPU\n",
        "\n",
        "> This will help keep the GPU Memory requirements under 20 GBs. Suitable for L4 GPU"
      ],
      "metadata": {
        "id": "YN0DpcXUC1VC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=128,\n",
        "    lora_alpha=256,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    use_rslora=True\n",
        ")\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,  # Each GPU processes 2 examples per step.\n",
        "    gradient_accumulation_steps=1,  # Gradients are accumulated over 1 steps before updating weights.\n",
        "    # warmup_steps=30,  # Learning rate warms up (gradually increases) for the first 30 steps.\n",
        "    #max_steps=10,  # Total number of optimization steps for training.\n",
        "    warmup_ratio=0.1, # Learning rate warms up (gradually increases) for the first 10 percent of epoch.\n",
        "    num_train_epochs=1,  # Total number of epochs for training.\n",
        "    gradient_checkpointing=True,  # Saves memory by recomputing activations during backpropagation.\n",
        "    learning_rate=5e-5,  # Base learning rate for the optimizer.\n",
        "    fp16=not torch.cuda.is_bf16_supported(),  # FP16 precision if BF16 is not available.\n",
        "    bf16=torch.cuda.is_bf16_supported(),  # Enables bfloat16 precision if available.\n",
        "    save_steps=100,  # Saves checkpoint every 100 steps.\n",
        "    torch_empty_cache_steps = 100,  # Empties the cache at every 100 steps.\n",
        "    optim=\"adamw_8bit\",  # Uses AdamW optimizer with 8-bit precision for optimizer states to save memory.\n",
        "    weight_decay=0.01,  # Regularization to prevent overfitting by penalizing large weights.\n",
        "    lr_scheduler_type=\"linear\",  # Linearly decays learning rate after the warmup period.\n",
        "    output_dir=\"gemma-2-2b-{hi)-alpaca-chk\",  # Directory where model checkpoints and logs will be saved.\n",
        "    report_to=\"none\",  # Disables logging to external tools like TensorBoard or WandB.\n",
        "    save_total_limit=2, # Will save only 2 checkpoints at max, reducing the disk usage.\n",
        "    run_name='pretrain_gemma2' # Defining a name for our runtime.\n",
        ")"
      ],
      "metadata": {
        "id": "2GuW1DgJ86pk",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> NOTE: In both tokenizer and collator some arguments are commmon.\n",
        "- **padding** 'longest' to find the longest batch and pad based on that\n",
        "- **padding** 'max_length' will then require u to set a\n",
        "  - **max_length** parameter for padding,\n",
        "  - **truncation** parameter to cut any sequences longer than max_length"
      ],
      "metadata": {
        "id": "UE_FXHMzEwmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you did not tokenized the dataset, you must use Data Collator.\n",
        "# It uses tokenizer, tokenize your training data and returns them as tensors.\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=base_model,\n",
        "    padding=\"longest\",\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=train_args,\n",
        "    peft_config=lora_config,\n",
        "    train_dataset=dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "rYWa-oEwCQtm",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# To begin training use\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Y712uEr-O9vf",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# To resume training from last checkpoint use\n",
        "trainer.train(resume_from_checkpoint=True)"
      ],
      "metadata": {
        "id": "u6h_r85JFjqA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Once training is done save the model and the tokenizer\n",
        "trainer.save_model('gemma-2-2b-(hi)-24985steps-1epoch-alphacha')\n",
        "trainer.tokenzier.save_pretrained('gemma-2-2b-(hi)-24985steps-1epoch-alphacha')"
      ],
      "metadata": {
        "id": "aaqhesCmGUhq",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: The training loss as you can see gets low quickly and can go even lower as we training it for more epochs"
      ],
      "metadata": {
        "id": "b0_f5OsRSK79"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Inference"
      ],
      "metadata": {
        "id": "x4yS5rabFsYV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For inference you will need to merge the base model and adapter model like this\n",
        "\n",
        "> NOTE: Always merge a non quantized base model with the adapter to avoid rounding errors during inference and causing unexpected behaviour"
      ],
      "metadata": {
        "id": "nGQg6xJCGD7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained('gemma-2-2b', device_map=\"cpu\"), 'gemma-2-2b-24985steps-1epoch-alphacha')"
      ],
      "metadata": {
        "id": "vBjBLh5d4rG0",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"कुछ एक रीसाइक्लिंग अभियान के लिए एक नारा सुझाव दें।\"\n",
        "\n",
        "\n",
        "\n",
        "inputs = tokenizer(question, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=128,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "2IJbkR_cjvaf",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see how the model is now able to answer to a hindi query that according to the dataset it has learned from\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gg132rLuH2E0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model as transformer file"
      ],
      "metadata": {
        "id": "1DIg-qViI-j7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: These steps are very curcial for ensuring the model weights are properly transfered as without this we faced an inference loss where our saved pretrained model wasn't able to infer properly without the state dictionary weight transfer so we made it a default saving frame work for us"
      ],
      "metadata": {
        "id": "NgipDLivJQR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.save_pretrained(\"gemma-2-2b-tmp\")"
      ],
      "metadata": {
        "id": "Prr1H1s8NlJr",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(merged_model.state_dict(), \"merged_model_state_dict.pth\")"
      ],
      "metadata": {
        "id": "qe9Ap0Pmr0Vz",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"gemma-2-2b-tmp\",device_map='cpu')"
      ],
      "metadata": {
        "id": "3KCqAjYYQuC4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"merged_model_state_dict.pth\", weights_only=True))"
      ],
      "metadata": {
        "id": "CJysQWlIsjDc",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gemma-2-2b-{hi)-24985steps-1epoch-alphacha\")"
      ],
      "metadata": {
        "id": "D5qM_ZksRtQj",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"gemma-2-2b-base+alpaca\")"
      ],
      "metadata": {
        "id": "Vy5MdISt1kkG",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"gemma-2-2b-base+alpaca\")"
      ],
      "metadata": {
        "id": "5MGSAKoB1m8h",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Saved model inference -"
      ],
      "metadata": {
        "id": "ptLz5eu7Kc7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"कुछ एक रीसाइक्लिंग अभियान के लिए एक नारा सुझाव दें।\"\n",
        "\n",
        "inputs = tokenizer(question, return_tensors=\"pt\").to('cpu')\n",
        "\n",
        "generated_ids = model.generate(**inputs,\n",
        "                              max_new_tokens=128,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1,\n",
        "                              use_cache=False)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "ta14FrfZ5Gfq",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## 3. Fine tuning on Congitive Lab Hindi Instruct Dataset"
      ],
      "metadata": {
        "id": "oRiNWvGgKkiu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We decided to further fine tune our model on another dataset to test what kind of changes will that bring to our model and possibly increase its knowledge base further (it didn't go very well)\n",
        "\n",
        "Here's where we learnt the importance of definining the perfect LoRA Configs to train or fine tune any LLM\n",
        "\n",
        "We trained for a total of **20 Hrs** on this dataset\n",
        "\n",
        "> NOTE: We will always load a quantized model for training."
      ],
      "metadata": {
        "id": "hkwbdguNK2H8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "Ts49hkYSNjAd",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gemma-2-2b-base+alpaca\")"
      ],
      "metadata": {
        "id": "YRryC87jbeKw",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"gemma-2-2b-base+alpaca\",quantization_config=bnb_config,\n",
        "                                                                         device_map='auto')"
      ],
      "metadata": {
        "id": "uMDUdRK0K7_g",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> NOTE: In this data set as we can see they tried to help the model understand the previous user-model interactions which can be useful if you want to instruction tune it to understand how to use previous conversations when you are using History in an LLM\n",
        "\n",
        "> However there is a major issue in the training prompt. Can you see it? We accidentally trained our model on this dataset without rectifying the issue which led to a big problem during inference which we will see ahead\n",
        "\n",
        "> We will also delve into ways of resolving that issue in a lot cheaper way"
      ],
      "metadata": {
        "id": "Sp2ZRLtsUDS8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Preparation"
      ],
      "metadata": {
        "id": "7wecnQtVmxbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cognitive_hi_inst_train = load_dataset(\"CognitiveLab/Hindi-Instruct\", split='train')\n",
        "cognitive_hi_inst_test = load_dataset(\"CognitiveLab/Hindi-Instruct\", split='test')"
      ],
      "metadata": {
        "id": "ETU480gLJNPP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "cognitive_hi_inst_train, print(cognitive_hi_inst_train[0]['text']), print(cognitive_hi_inst_train[0]['input_ids'])"
      ],
      "metadata": {
        "id": "RmkDhrZ6s-Pv",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tokenizer is more or less the same as we used for the other dataset"
      ],
      "metadata": {
        "id": "qTk4aAjlW0jH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "train_dataset = cognitive_hi_inst_train.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "test_dataset = cognitive_hi_inst_test.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
        "print(\"Dataset tokenized:\", train_dataset[0])"
      ],
      "metadata": {
        "id": "EFebaHl2JRpL",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "9JhgYnvuTGmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> NOTE: This time we decided to\n",
        "- only focus on the attention and feed forward layers and\n",
        "- exclude the embedding and lm layers while training.\n",
        "\n",
        "> We also reduced\n",
        "- **\"r\"** and\n",
        "- lora_alpha\n",
        "- torch empty cache parameter to save more vram for larger batch size\n",
        "\n",
        "> We will see its consequences ahead"
      ],
      "metadata": {
        "id": "KgjsLGTEXDVe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=128,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "                    ],\n",
        "    #modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    use_rslora=True\n",
        ")\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    per_device_train_batch_size=3,  # Each GPU processes 4 examples per step.\n",
        "    gradient_accumulation_steps=1,  # Gradients are accumulated over 4 steps before updating weights.\n",
        "    # warmup_steps=30,  # Learning rate warms up (gradually increases) for the first 30 steps.\n",
        "    #max_steps=10,  # Total number of optimization steps for training.\n",
        "    warmup_ratio=0.1, # Learning rate warms up (gradually increases) for the first 10 percent of epoch.\n",
        "    num_train_epochs=1,  # Total number of epochs for training.\n",
        "    gradient_checkpointing=True,  # Saves memory by recomputing activations during backpropagation.\n",
        "    learning_rate=5e-5,  # Base learning rate for the optimizer.\n",
        "    fp16=not torch.cuda.is_bf16_supported(),  # FP16 precision if BF16 is not available.\n",
        "    bf16=torch.cuda.is_bf16_supported(),  # Enables bfloat16 precision if available.\n",
        "    save_steps=100,  # Saves checkpoint every 100 steps.\n",
        "    torch_empty_cache_steps=10,  # Empties the cache at every 10 steps.\n",
        "    optim=\"adamw_8bit\",  # Uses AdamW optimizer with 8-bit precision for optimizer states to save memory.\n",
        "    weight_decay=0.01,  # Regularization to prevent overfitting by penalizing large weights.\n",
        "    lr_scheduler_type=\"linear\",  # Linearly decays learning rate after the warmup period.\n",
        "    output_dir=\"gemma-2-2b-cog-lab-chk\",  # Directory where model checkpoints and logs will be saved.\n",
        "    report_to=\"none\",  # Disables logging to external tools like TensorBoard or WandB.\n",
        "    save_total_limit=2, # Will save only 2 checkpoints at max, reducing the disk usage.\n",
        "    run_name='pretrain_gemma2' # Defining a name for our runtime.\n",
        ")"
      ],
      "metadata": {
        "id": "RiNbt7Fx7AmX",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=base_model,\n",
        "    padding=\"longest\",\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=train_args,\n",
        "    peft_config=lora_config,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "yQEK1gXI7YBA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> NOTE: Notice now how the training loss didn't change too drastically unlike the last time. This can be the result of the LoRA Config, some hyper parameters or the dataset itself"
      ],
      "metadata": {
        "id": "VogTkoE8Y1ur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "Sq_fH-fJYzCJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Follow the exact same saving steps"
      ],
      "metadata": {
        "id": "BFgedJc9ZMpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"gemma-2-2b-30443steps-1epoch-cog-lab\")"
      ],
      "metadata": {
        "id": "QR9sUcSW8skK",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.tokenizer.save_pretrained('gemma-2-2b-30443steps-1epoch-cog-lab')"
      ],
      "metadata": {
        "id": "aG5ihcI0MvQH",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "AldmN45nZhJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the merge the base and adapter"
      ],
      "metadata": {
        "id": "AYnBggO0ZpaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(\"gemma-2-2b-base+alpaca\",device_map='cpu'), 'gemma-2-2b-30443steps-1epoch-cog-lab').merge_and_unload()"
      ],
      "metadata": {
        "id": "0yMo6rcqMflY",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"<start_of_turn>user क्या आप मुझे रीसाइक्लिंग के लिए एक नारा समझा सकते हैं? <end of turn>\\n<start_of_turn>model \"\n",
        "\n",
        "\n",
        "inputs = tokenizer(question, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=246,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1,\n",
        "                              use_cache=False)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
      ],
      "metadata": {
        "id": "Eb-13pq5NXnC",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> NOTE: Notice how the model is unable to stop its generation unlike the last time. Can you guess why this is happening?\n",
        "\n",
        "> That's right, the end of sentence token. If you notice in our training prompt there was no `<eos>` token. This means the model didn't learn when to stop it's generation and keeps on continuing with more tokens.\n",
        "\n",
        "> To fix this we simply need to add the `<bos>` and `<eos>` tokens to our prompt to tell the model what is the begenning and the end of a conversation"
      ],
      "metadata": {
        "id": "GVFHIBKAZx0m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model\n",
        "\n",
        "Ofc first we will save our model like before"
      ],
      "metadata": {
        "id": "rqr99pPDfROr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.save_pretrained(\"gemma-2-2b-tmp\")"
      ],
      "metadata": {
        "id": "23tQ4OpufV_b",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(merged_model.state_dict(), \"merged_model_state_dict.pth\")"
      ],
      "metadata": {
        "id": "oGuk8QNJfV_c",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"gemma-2-2b-tmp\",device_map='cpu')"
      ],
      "metadata": {
        "id": "pf3kkKBnfV_c",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"merged_model_state_dict.pth\", weights_only=True))"
      ],
      "metadata": {
        "id": "Q1i3XqKEfV_c",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gemma-2-2b-30443steps-1epoch-cog-lab\")"
      ],
      "metadata": {
        "id": "oxrFBd2_fV_c",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"gemma-2-2b-base+alpaca+cog-lab\")"
      ],
      "metadata": {
        "id": "A6HYJpq8fV_d",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"gemma-2-2b-base+alpaca+cog-lab\")"
      ],
      "metadata": {
        "id": "ZJP-yrspfV_d",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Further Fine Tuning to teach beginning and end tokens"
      ],
      "metadata": {
        "id": "U4sZdzJhkABN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_prompt_with_tokens(batch):\n",
        "    formatted_prompts = []\n",
        "    for conversations in batch['messages']:\n",
        "        formatted_prompt = []\n",
        "        for i in range(0, len(conversations), 2):  # Process user-model pairs\n",
        "            user_message = conversations[i]\n",
        "            model_message = conversations[i + 1] if i + 1 < len(conversations) else None\n",
        "\n",
        "            if user_message['role'] == \"user\" and model_message and model_message['role'] == \"assistant\":\n",
        "                formatted_prompt.append(\n",
        "                    f\"<bos><start_of_turn>{user_message['role']} {user_message['content']} <end_of_turn>\\n\"\n",
        "                    f\"<start_of_turn>model {model_message['content']} <end_of_turn><eos>\"\n",
        "                )\n",
        "\n",
        "        # Join the formatted prompt for this conversation\n",
        "        formatted_prompts.append(\"\\n\".join(formatted_prompt))\n",
        "\n",
        "    # Return the formatted text as a new field in the dataset\n",
        "    return {\"text\": formatted_prompts}"
      ],
      "metadata": {
        "id": "detYgOHGNOAA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> NOTE: This time we take 3000 samples for fine tuning the model to teach it when to use `<eos>` token"
      ],
      "metadata": {
        "id": "kefm3skDPVst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the dataset and take 1000 examples\n",
        "random_subset = cognitive_hi_inst_train.take(3000)\n",
        "\n",
        "# Apply the formatting function to this subset\n",
        "cognitive_hi_inst_dataset = random_subset.map(format_prompt_with_tokens, batched=True)"
      ],
      "metadata": {
        "id": "hrOj1HZJPNoK",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(cognitive_hi_inst_dataset[1]['text'])"
      ],
      "metadata": {
        "id": "vGrm7UbNPNoQ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    tokenizer.padding_side = \"right\"\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        max_length=1024,\n",
        "        truncation=True,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "train_dataset = cognitive_hi_inst_dataset.map(tokenize_function, batched=True)\n",
        "print(\"Dataset tokenized:\", train_dataset[0])"
      ],
      "metadata": {
        "id": "bjfLcnitbDcI",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> NOTE: The rest of the things are the same.\n",
        "- We added a small dropout to try to prevent content bias\n"
      ],
      "metadata": {
        "id": "fUcazDvSQiEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=128,\n",
        "    lora_alpha=256,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\",\"gate_proj\", \"up_proj\", \"down_proj\",],\n",
        "    lora_dropout=0.05,\n",
        "    #modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    use_rslora=True\n",
        ")\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    per_device_train_batch_size=3,  # Each GPU processes 4 examples per step.\n",
        "    gradient_accumulation_steps=1,  # Gradients are accumulated over 4 steps before updating weights.\n",
        "    warmup_steps=30,  # Learning rate warms up (gradually increases) for the first 30 steps.\n",
        "    #max_steps=1000,  # Total number of optimization steps for training.\n",
        "    warmup_ratio=0.1, # Learning rate warms up (gradually increases) for the first 10 percent of epoch.\n",
        "    num_train_epochs=1,  # Total number of epochs for training.\n",
        "    gradient_checkpointing=True,  # Saves memory by recomputing activations during backpropagation.\n",
        "    learning_rate=5e-6,  # Base learning rate for the optimizer.\n",
        "    fp16=not torch.cuda.is_bf16_supported(),  # FP16 precision if BF16 is not available.\n",
        "    bf16=torch.cuda.is_bf16_supported(),  # Enables bfloat16 precision if available.\n",
        "    save_steps=100,  # Saves checkpoint every 100 steps.\n",
        "    torch_empty_cache_steps=10,  # Empties the cache at every 10 steps.\n",
        "    logging_steps=100,  # Logs metrics every 100 steps.\n",
        "    optim=\"adamw_8bit\",  # Uses AdamW optimizer with 8-bit precision for optimizer states to save memory.\n",
        "    weight_decay=0.01,  # Regularization to prevent overfitting by penalizing large weights.\n",
        "    lr_scheduler_type=\"linear\",  # Linearly decays learning rate after the warmup period.\n",
        "    output_dir=\"gemma-2-2b-(hi)-cog-lab-chk-fnt\",  # Directory where model checkpoints and logs will be saved.\n",
        "    report_to=\"none\",  # Disables logging to external tools like TensorBoard or WandB.\n",
        "    save_total_limit=2, # Will save only 2 checkpoints at max, reducing the disk usage.\n",
        "    run_name='pretrain_gemma2' # Defining a name for our runtime.\n",
        ")"
      ],
      "metadata": {
        "id": "PcNOCCYmjBok",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=base_model,\n",
        "    padding=\"longest\",\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=train_args,\n",
        "    peft_config=lora_config,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "lgbtaBAgjBol",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "2v5_8FCpbl-A",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training it for more epochs could've shown a better result"
      ],
      "metadata": {
        "id": "feiR6zzCRQxr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"gemma-2-2b-1000steps-0.03epoch-cog-lab-fnt\")"
      ],
      "metadata": {
        "id": "Bw8R8jTl0JMn",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.tokenizer.save_pretrained(\"gemma-2-2b-1000steps-0.03epoch-cog-lab-fnt\")"
      ],
      "metadata": {
        "id": "LSwPvQ1F_jjy",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "As always we will merge the models\n",
        "\n"
      ],
      "metadata": {
        "id": "chGNPYQJRlX8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained(\"gemma-2-2b-(hi)-base-alpc-cb\",device_map='auto'), 'gemma-2-2b-it(hi)-1000steps-0.03epoch-cog-lab-fnt').merge_and_unload()"
      ],
      "metadata": {
        "id": "s1JVZvkg0hZU",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "It seems the model can now answer in Hinglish well enough, but has become pattern biased where in it thinks everything needs a thorough explanation along with its historical context"
      ],
      "metadata": {
        "id": "1s7WfymgR-9-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"<start_of_turn>user Tell me about elephants, but tell me in English please. <end of turn>\\n<start_of_turn>model \"\n",
        "\n",
        "\n",
        "inputs = tokenizer(question, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=246,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1,\n",
        "                              use_cache=False)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
      ],
      "metadata": {
        "id": "xg1QE05m0hZV",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "It can also answer in English with same pattern biasness"
      ],
      "metadata": {
        "id": "it1UOaj2SpNv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"<start_of_turn>user What's your name? <end_of_turn>\\n<start_of_turn>\"\n",
        "\n",
        "\n",
        "inputs = tokenizer(question, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=246,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1,\n",
        "                              use_cache=False)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
      ],
      "metadata": {
        "id": "rJ92ajr72wZG",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Since it was trained on multi-turn dataset we decided to check whether that type of prompt format can help it"
      ],
      "metadata": {
        "id": "R5yl6Z5VTF71"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"<start_of_turn>user सुबेह सुबेह उठने वाली चिड़िया कौनसी है? <end of turn>\\n<start_of_turn>model सुबह-सुबह बहुत सी चिड़िया उठती है, आपको किसके बारे में जानना है? <end_of_turn>\\n<start_of_turn>user आपको कौनसी चिड़ियाँ के बारे में पता है? <end of turn>\\n<start_of_turn>model \"\n",
        "\n",
        "\n",
        "inputs = tokenizer(question, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=246,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1,\n",
        "                              use_cache=False)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
      ],
      "metadata": {
        "id": "ZTIEMpn-hLJF",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "We tried **Few Shot** examples to tell it how to have a conversation and it worked well enough but although the content was sub optimal"
      ],
      "metadata": {
        "id": "2qvx-ECnSxU-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"The following is a conversation between a user and model. The assistant responds in Hindi and provides accurate, concise answers.\\nExample 1:\\n<start_of_turn>user भारत की राजधानी क्या है? <end_of_turn>\\n<start_of_turn>model भारत की राजधानी नई दिल्ली है। <end_of_turn>\\nExample 2:\\n<start_of_turn>user पिरामिड कहां पाए जाते हैं? <end_of_turn>\\n<start_of_turn>model पिरामिड मुख्य रूप से मिस्र में पाए जाते हैं, लेकिन सूडान, मेसोअमेरिका और इटली जैसे अन्य स्थानों पर भी हैं। <end_of_turn>\\nExample 3:<start_of_turn>user मुझे चाय और कॉफी के फायदे बताओ। <end_of_turn>\\n<start_of_turn>model चाय एंटीऑक्सिडेंट्स से भरपूर होती है और तनाव कम करती है। वहीं, कॉफी सतर्कता और ऊर्जा को बढ़ाती है। <end_of_turn>\\nNow continue the conversation:\\n<start_of_turn>user भारत के पड़ोसी देशों के नाम क्या हैं? <end_of_turn>\\n<start_of_turn>model \"\n",
        "\n",
        "\n",
        "inputs = tokenizer(question, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=246,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1,\n",
        "                              use_cache=False)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
      ],
      "metadata": {
        "id": "tfrfJQrfgWng",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Conclusion: But as you can see, the model has now learnt to produce the end of turn and end of sentence tokens along with the knowledge of when to stop the generation.\n",
        "\n",
        "This goes to show us it is actually fairly easy to teach a model the turn based approach and eos token generation with a small amount of training data requiring much lesser resources.\n",
        "\n",
        "The model's performance in specific tasks can be improved with careful tweaking of the LoRA parameters which greatly impacts the training\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "ural7CBkTe5d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training on Combined Mixed Text Corpus"
      ],
      "metadata": {
        "id": "Ssq_clj1c4x9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this part of our research we did something different. So far we decided to train our model on different available datasets one by one. Which did give us some result specially when it came to teaching the model how to use genearal hindi tokens. With some achievements in the code switching and hinglish capabilities.\n",
        "\n",
        "This time we decided to improve the model's knowledge base along with teaching it how to communicate in Hindi, Hinglish and possibly code switching by giving specific purpose datasets for the said tasks.\n",
        "\n",
        "We gave a mixture of following\n",
        "- **Wikipedia** datasets in hindi and hinglish\n",
        "- **Alpaca** Hindi conversation dataset\n",
        "- **Databricks Dolly** code mix , hinglish instruct dataset\n",
        "- **Hindi Math Quest** dataset for hindi mathematics\n",
        "\n",
        "Although our main goal was to increase the model's knowledge base in hindi, hinglish and code switching, we added maths for the possibility of increasing it's reasoning capability\n",
        "\n",
        "We trained for a total of only **4 Hrs** on this dataset as unfortunately our resources had run out.\n",
        "\n",
        "We'll see some interesting results ahead\n",
        "\n",
        "We'll begin with downloading and carefully formating our datasets, putting them in right prompt. We only took a small subset of all the datasets carefully in order to induce certain behaviour from each as per resource and time constraints"
      ],
      "metadata": {
        "id": "VVg_wrJPc-z_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Datasets Preparation\n",
        "For wikipedia, we used the **titles** as **user** query and **texts** as **model** output"
      ],
      "metadata": {
        "id": "0DGChkDogbBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_1 = load_dataset(\"Cohere/wikipedia-22-12-hi-embeddings\", split = \"train\",)\n",
        "wiki_1[0]['title'], wiki_1[0]['text']"
      ],
      "metadata": {
        "id": "pBhT5oAW2AZ-",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_2 = load_dataset(\"wikimedia/wikipedia\", \"20231101.hi\", split = \"train\",)\n",
        "wiki_2[0]['title'], wiki_2[0]['text']"
      ],
      "metadata": {
        "id": "jfnea8Zlv2d2",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_3 = load_dataset(\"sgzsh269/wikipedia-hindi-hinglish\", split = \"train\",)\n",
        "wiki_3[0]['hindi_title'], wiki_3[0]['hindi_text'], wiki_3[0]['hinglish_title'], wiki_3[0]['hinglish_text']"
      ],
      "metadata": {
        "id": "XKlisIf66OZj",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_3"
      ],
      "metadata": {
        "id": "Ua01j2Lf_oxt",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def format_func_d1(example):\n",
        "    prompts = []\n",
        "    titles = example[\"title\"]\n",
        "    texts = example[\"text\"]\n",
        "\n",
        "    # Loop over each example in the batch\n",
        "    for title, text in zip(titles, texts):\n",
        "        prompt = f\"<start_of_turn>user: {title} <end_of_turn>\\n<start_of_turn>model: {text}<end_of_turn><eos>\"\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Return as a batch\n",
        "    return {\"prompt\": prompts}\n",
        "\n",
        "wiki_1_train = wiki_1.select_columns([\"title\",\"text\"]).shuffle().take(5000).map(format_func_d1, batched=True).select_columns([\"prompt\"])\n",
        "wiki_1_train[0], wiki_1_train[0]['prompt']"
      ],
      "metadata": {
        "id": "jC-TxttB-a4_",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def format_func_d2(example):\n",
        "    prompts = []\n",
        "    titles = example[\"title\"]\n",
        "    texts = example[\"text\"]\n",
        "\n",
        "    # Loop over each example in the batch\n",
        "    for title, text in zip(titles, texts):\n",
        "        prompt = f\"<start_of_turn>user: {title} <end_of_turn>\\n<start_of_turn>model: {text}<end_of_turn><eos>\"\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Return as a batch\n",
        "    return {\"prompt\": prompts}\n",
        "\n",
        "wiki_2_train = wiki_2.select_columns([\"title\",\"text\"]).shuffle().take(5000).map(format_func_d2, batched=True).select_columns([\"prompt\"])\n",
        "wiki_2_train[0]['prompt']"
      ],
      "metadata": {
        "id": "ddz4HTkADh-r",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def format_func_d3(example):\n",
        "    prompts = []\n",
        "    try: titles = example[\"hindi_title\"]\n",
        "    except: titles = example[\"hinglish_title\"]\n",
        "    try: texts = example[\"hindi_text\"]\n",
        "    except: texts = example[\"hinglish_text\"]\n",
        "\n",
        "    # Loop over each example in the batch\n",
        "    for title, text in zip(titles, texts):\n",
        "        prompt = f\"<start_of_turn>user: {title} <end_of_turn>\\n<start_of_turn>model: {text}<end_of_turn><eos>\"\n",
        "        prompts.append(prompt)\n",
        "\n",
        "    # Return as a batch\n",
        "    return {\"prompt\": prompts}\n",
        "\n",
        "wiki_3_train_hi = wiki_3.select_columns([\"hindi_title\",\"hindi_text\"]).map(format_func_d3, batched=True).select_columns([\"prompt\"])\n",
        "wiki_3_train_he = wiki_3.select_columns([\"hinglish_title\",\"hinglish_text\"]).map(format_func_d3, batched=True).select_columns([\"prompt\"])\n",
        "wiki_3_train = concatenate_datasets([wiki_3_train_hi, wiki_3_train_he])\n",
        "wiki_3_train_hi[0]['prompt'], wiki_3_train_he[0]['prompt']"
      ],
      "metadata": {
        "id": "uDFMe0lZD-3t",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "wiki_dataset_train = concatenate_datasets([wiki_1_train, wiki_2_train, wiki_3_train])\n",
        "wiki_dataset_train"
      ],
      "metadata": {
        "id": "QC_DvWreHGpT",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> NOTE: This alpaca dataset is different from the previous one.\n",
        "\n",
        "We have given both the instruction and input fields as user query."
      ],
      "metadata": {
        "id": "2ylMtiRuhk_r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_dataset_train = load_dataset(\"guneetsk99/Hindi_Alpaca_For_Gemma_67K\",\n",
        "                              split = \"train\")\n",
        "alpaca_dataset_train, alpaca_dataset_train[3]"
      ],
      "metadata": {
        "id": "C9iHx0mugRvg",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_dataset_train"
      ],
      "metadata": {
        "id": "TeFhqK_VgRvg",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "gen_prompt=\"\"\"<start_of_turn>user: {} {}<end_of_turn>\\n<start_of_turn>model: {}<end_of_turn>\"\"\"\n",
        "print(alpaca_prompt)"
      ],
      "metadata": {
        "id": "fil0RRYwgRvg",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(examples):\n",
        "    prompts = []\n",
        "    instruction = examples[\"instruction\"]\n",
        "    input = examples[\"input\"]\n",
        "    output = examples['output']\n",
        "    # Loop over each example in the batch\n",
        "    for instruction, input, output in zip(instruction, input, output):\n",
        "        input = input if input else ''\n",
        "        prompt = gen_prompt.format(instruction, input, output) + '<eos>'\n",
        "        prompts.append(prompt)\n",
        "    return { \"prompt\" : prompts, }"
      ],
      "metadata": {
        "id": "0hv9OwmggRvg",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_train = alpaca_dataset_train.shuffle().take(2000).map(formatting_func, batched = True,).select_columns([\"prompt\"])"
      ],
      "metadata": {
        "id": "TY_oRvvrgRvh",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "alpaca_train,alpaca_train[0]"
      ],
      "metadata": {
        "id": "r0P2dHWjgRvh",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "print(alpaca_train[\"prompt\"][0])"
      ],
      "metadata": {
        "id": "-3ZetGHhgRvh",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "databrick_dolly = load_dataset(\"aaditya/databricks-dolly-15k-Hinglish-Codemix\", split = \"train\")"
      ],
      "metadata": {
        "id": "3NAXzZbVJnoe",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "databrick_dolly[0]"
      ],
      "metadata": {
        "id": "EJ5eqxFWKEG0",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(examples):\n",
        "    prompts = []\n",
        "    instruction = examples[\"codemix_instruction\"]\n",
        "    input = examples[\"codemix_input\"]\n",
        "    output = examples['codemix_output']\n",
        "    # Loop over each example in the batch\n",
        "    for instruction, input, output in zip(instruction, input, output):\n",
        "        instruction = instruction if instruction else ''\n",
        "        if instruction:\n",
        "          input = f'{input}' if input else ''\n",
        "        else:\n",
        "          input = input if input else ''\n",
        "        prompt = gen_prompt.format(instruction, input, output) + '<eos>'\n",
        "        prompts.append(prompt)\n",
        "    return { \"prompt\" : prompts, }"
      ],
      "metadata": {
        "id": "7yyLSTUzJ-zG",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "databrick_train = databrick_dolly.shuffle().take(2000).map(formatting_func, batched = True,).select_columns([\"prompt\"])"
      ],
      "metadata": {
        "id": "AtOyU2u-LU3D",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "databrick_train, databrick_train[0]"
      ],
      "metadata": {
        "id": "O8yd8s12KB8F",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "> NOTE: For certain gated datasets you need to login to HuggingFace or activate your hf token which you can get from your HuggingFace account"
      ],
      "metadata": {
        "id": "_pHep5F4kXCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "metadata": {
        "id": "58Rg0D0qM56K",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"HF_HOME\"] = \"your_hf_token\""
      ],
      "metadata": {
        "id": "zng9KqNyM6Vy",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "math_quest = load_dataset(\"dnyanesh/HindiMathQuest\", split = \"train\")\n",
        "math_quest[0]"
      ],
      "metadata": {
        "id": "FccHrx6VMBS1",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def formatting_func(examples):\n",
        "    prompts = []\n",
        "    instruction = examples[\"instruction\"]\n",
        "    input = examples[\"input\"]\n",
        "    output = examples['output']\n",
        "    # Loop over each example in the batch\n",
        "    for instruction, input, output in zip(instruction, input, output):\n",
        "        input = input if input else ''\n",
        "        prompt = gen_prompt.format(instruction, input, output) + '<eos>'\n",
        "        prompts.append(prompt)\n",
        "    return { \"prompt\" : prompts, }"
      ],
      "metadata": {
        "id": "5W9hey31NRVF",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "mathquest_train = math_quest.shuffle().take(2000).map(formatting_func, batched = True,).select_columns([\"prompt\"])"
      ],
      "metadata": {
        "id": "oLMP67qnNVud",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "mathquest_train, mathquest_train[0]"
      ],
      "metadata": {
        "id": "Cjho8acKNjDW",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Finally we will concatenate the datasets and tokenize them"
      ],
      "metadata": {
        "id": "3y5hqPc5tD0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = concatenate_datasets([wiki_dataset_train, alpaca_train, databrick_train, mathquest_train]).shuffle()\n",
        "train_dataset, train_dataset[0]"
      ],
      "metadata": {
        "id": "F9wfZARElOxX",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    tokenized = tokenizer(\n",
        "        examples[\"prompt\"],\n",
        "        padding=\"longest\",\n",
        "        truncation=True,\n",
        "        max_length=1024,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()\n",
        "    return tokenized\n",
        "\n",
        "print(\"Tokenizing dataset...\")\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "print(\"Dataset tokenized:\", train_dataset[0])"
      ],
      "metadata": {
        "id": "mSDY5fF8k_SU",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset"
      ],
      "metadata": {
        "id": "cToQ2wglmgUv",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "_IRqL8iItOMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "NOTE: As always the lora parameters are very crucial.\n",
        "\n",
        "- Analyze our current parameters as compared to our previous configs.\n",
        "-You can increase the **r** and alpha to prompt even more layers with stronger learning.\n",
        "- We have set the gradient accumulation to 2 this time."
      ],
      "metadata": {
        "id": "P3T2LQTtlRkY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=64,\n",
        "    lora_alpha=128,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "                    ],\n",
        "    modules_to_save=[\"embed_tokens\", \"lm_head\"],\n",
        "    task_type=\"CAUSAL_LM\",\n",
        "    use_rslora=True\n",
        ")\n",
        "\n",
        "train_args = TrainingArguments(\n",
        "    per_device_train_batch_size=2,  # Each GPU processes 2 examples per step.\n",
        "    gradient_accumulation_steps=2,  # Gradients are accumulated over 2 steps before updating weights.\n",
        "    # warmup_steps=30,  # Learning rate warms up (gradually increases) for the first 30 steps.\n",
        "    #max_steps=10,  # Total number of optimization steps for training.\n",
        "    warmup_ratio=0.1, # Learning rate warms up (gradually increases) for the first 10 percent of epoch.\n",
        "    num_train_epochs=1,  # Total number of training steps for training.\n",
        "    gradient_checkpointing=True,  # Saves memory by recomputing activations during backpropagation.\n",
        "    learning_rate=5e-5,  # Base learning rate for the optimizer.\n",
        "    fp16=not torch.cuda.is_bf16_supported(),  # FP16 precision if BF16 is not available.\n",
        "    bf16=torch.cuda.is_bf16_supported(),  # Enables bfloat16 precision if available.\n",
        "    save_steps=100,  # Saves checkpoint every 100 steps.\n",
        "    torch_empty_cache_steps = 10,  # Empties the cache at every 10 steps.\n",
        "    logging_steps=100,  # Logs metrics every 10 steps.\n",
        "    optim=\"adamw_8bit\",  # Uses AdamW optimizer with 8-bit precision for optimizer states to save memory.\n",
        "    weight_decay=0.01,  # Regularization to prevent overfitting by penalizing large weights.\n",
        "    lr_scheduler_type=\"linear\",  # Linearly decays learning rate after the warmup period.\n",
        "    output_dir=\"gemma-2-2b-(hi)-wiki+alpaca+databrick+mathquest_chk\",  # Directory where model checkpoints and logs will be saved.\n",
        "    report_to=\"none\",  # Disables logging to external tools like TensorBoard or WandB.\n",
        "    save_total_limit=2, # Will save only 2 checkpoints at max, reducing the disk usage.\n",
        "    run_name='pretrain_gemma2' # Defining a name for our runtime.\n",
        ")"
      ],
      "metadata": {
        "id": "_jL0c5cWk_SU",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=base_model,\n",
        "    padding=\"longest\",\n",
        "    return_tensors=\"pt\"\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=base_model,\n",
        "    tokenizer=tokenizer,\n",
        "    args=train_args,\n",
        "    peft_config=lora_config,\n",
        "    train_dataset=train_dataset,\n",
        "    data_collator=data_collator,\n",
        ")"
      ],
      "metadata": {
        "id": "N8VpbGPdk_SU",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "fkYZm9qroV9o",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model\n",
        "\n",
        "The saving and merging steps are the same as always"
      ],
      "metadata": {
        "id": "JWsHu5Yatei1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model('gemma-2-2b-{hi)-16994batch-1epoch-wiki+alpaca+databrick+mathquest')\n",
        "trainer.tokenizer.save_pretrained('gemma-2-2b-{hi)-16994batch-1epoch-wiki+alpaca+databrick+mathquest')"
      ],
      "metadata": {
        "id": "Mkktz2frU1kX",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('gemma-2-2b-{hi)-16994batch-1epoch-wiki+alpaca+databrick+mathquest')\n",
        "merged_model = PeftModel.from_pretrained(AutoModelForCausalLM.from_pretrained('gemma-2-2b', device_map='auto'), 'gemma-2-2b-{hi)-16994batch-1epoch-wiki+alpaca+databrick+mathquest').merge_and_unload()"
      ],
      "metadata": {
        "id": "2Fmhz-3_7754",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "merged_model.save_pretrained(\"gemma-2-2b-tmp\")"
      ],
      "metadata": {
        "id": "hiBZZeswtei2",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(merged_model.state_dict(), \"merged_model_state_dict.pth\")"
      ],
      "metadata": {
        "id": "RoNiA3Yhtei2",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"gemma-2-2b-tmp\",device_map='cpu')"
      ],
      "metadata": {
        "id": "RQhV7vSytei3",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"merged_model_state_dict.pth\", weights_only=True))"
      ],
      "metadata": {
        "id": "AO7zAcvUtei3",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gemma-2-2b-(hi)-16994batch-1epoch-wiki+alpaca+databrick+mathquest\")"
      ],
      "metadata": {
        "id": "kLt69-Thtei4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"gemma-2-2b-(hi)-base+wiki+alpaca+databrick+mathquest\")"
      ],
      "metadata": {
        "id": "54Q-UTdgtei4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.save_pretrained(\"gemma-2-2b-(hi)-base+wiki+alpaca+databrick+mathquest\")"
      ],
      "metadata": {
        "id": "P7P4sIXTtei4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "JFZy1ejWmZ9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish and English. You respond to users in a clear, and concise manner in the language of the user query. \\nआप जेम्मा2 हैं, एक मददगार, संवादी एआई सहायक। आप हिंदी, बोलचाल की हिंग्लिश और अंग्रेजी में विशेषज्ञ हैं। आप उपयोगकर्ताओं को उपयोगकर्ता की क्वेरी की भाषा में स्पष्ट और संक्षिप्त तरीके से जवाब देते हैं। \\naap jemmaa2 hain, ek madadagaar, sanvaadee eaee sahaayak. aap hindee, bolachaal kee hinglish aur angrejee mein visheshagy hain. aap upayogakartaon ko upayogakarta kee kveree kee bhaasha mein spasht aur sankshipt tareeke se javaab dete hain.\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: Why is diwali celebrated<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=500,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "w3iUTnxvQkjA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish and English. You respond to users in a clear, and concise manner in the language of the user query\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: दिवाली का त्यौहार क्यों मनाया जाता है, संचेप में बतायें?<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = system_prompt + '\\n' + user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=500,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "3IcnLCVbSDOd",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems the model has learnt how to give answer in points\n",
        "\n",
        "---\n",
        "> NOTE: The model is able to infer in hindi and english as per the user query however it needs to be explicitly mentioned with the system prompt.\n",
        "\n",
        "Use the system prompt if the model isn't giving accurate results specially during translation and hinglish responses.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "62Qtov3VodpL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish and English. You respond to users in a clear, and concise manner in the language of the user query\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: यह रहा एक गणित का प्रश्न हिंदी में: \\n**प्रश्न:** \\nएक रेलगाड़ी की लंबाई 120 मीटर है। वह 72 किमी/घंटा की गति से चल रही है। रेलगाड़ी को एक 240 मीटर लंबे पुल को पार करने में कितना समय लगेगा? \\n(उत्तर सेकंड में दें।)?<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = system_prompt + '\\n' + user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=1000,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "6TpxJqdXS6ox",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "> NOTE: For maths its better to use less temperature as mathematical reasoning doesn't rely on creativity rather a streamlined aproach"
      ],
      "metadata": {
        "id": "sn0FTaSIqVBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, AI assistant. You are an expert in Hindi, colloquial Hinglish and English communication. You respond to users in a clear, and concise manner in the language of the user query\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: प्रश्न: एक रेलगाड़ी की लंबाई 120 मीटर है। वह 72 किमी/घंटा की गति से चल रही है। रेलगाड़ी को एक 240 मीटर लंबे पुल को पार करने में कितना समय लगेगा? (उत्तर सेकंड में दें।) निर्देश: इस प्रश्न को पहले ध्यान से पढ़ें और पूरी तरह से समझें। इसके बाद, इसे चरणबद्ध तरीके से हल करें। प्रत्येक चरण में अपने निष्कर्ष स्पष्ट रूप से प्रस्तुत करें और अंत में उत्तर दें।<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = system_prompt + '\\n' + user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=2000,\n",
        "                              do_sample=True,\n",
        "                              repetition_penalty=1)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "KkVBi6VqXQiV",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mathematical reasoning isn't all that great yet :')\n",
        "\n",
        "Although that wasn't our goal anyway\n",
        "\n",
        "Now let's check for areas where creativity is needed."
      ],
      "metadata": {
        "id": "DnegMyX0qqvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, AI assistant. You are an expert in Hindi, colloquial Hinglish and English communication. You respond to users in a clear, and concise manner in the language of the user query\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: Kya aapko pata hay ki ek saal me kitne din hote hain?<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input =  user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=2000,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "mtFV48OXo2xq",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Well atleast it knows how many days are there in a year\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "X0d-fX__P8UY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, AI assistant. You are an expert in Hindi, colloquial Hinglish and English communication. You respond to users in a clear, and concise manner in the language of the user query\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: एक यादृच्छिक कविता उत्पन्न करें<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input =  user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=2000,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "-zvLr_YSaRH7",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems to know the general structure of a poem however the response isn't too great in terms of coherence and meaning\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "rvD6a95ArCLV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, AI assistant. You are an expert in Hindi, colloquial Hinglish and English communication. You respond to users in a clear, and concise manner in the language of the user query\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: महात्मा गांधी के बारे में 100 शब्दो में निबंद लिखें।<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input =  user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=2000,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "SIJrSlZ9bAvR",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For essay writing\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "nRdLbMiarWqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, AI assistant. You are an expert in Hindi, colloquial Hinglish and English communication. You respond to users in a clear, and concise manner in the language of the user query\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: Python प्रोग्रामिंग लैंग्वेज में एक 'हैलो वर्ल्ड' का कोड लिखा है।<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input =  user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=2000,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "iZH0c72Ibzh4",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here our query itself was incorrect however it seemed to know we are asking about something related to programming and it gave a good response in code switching\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "NpwaewBcrl1j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, AI assistant. You are an expert in Hindi, colloquial Hinglish and English communication. You respond to users in a clear, and concise manner in the language of the user query\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: Translate 'And when i decided to play outside, it started raining' to hindi<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = system_prompt + '\\n' + user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(merged_model.device)\n",
        "\n",
        "generated_ids = merged_model.generate(**inputs,\n",
        "                              max_new_tokens=2000,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "XDt-tydNpIiD",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For translation task we needed to include the system prompt for a good response.\n",
        "\n",
        "> NOTE: All of the above generations could be improved with an even better instruction and system prompt as well as tuning the model's generation parameters.\n",
        "\n",
        "And there's an added benifit of training on larger mixed dataset\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "tk1Yet3Kr60r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. But why base?"
      ],
      "metadata": {
        "id": "MM9DT0jwrC_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You might be wondering why did we choose the base model eventually making it instruct, instead of choosing a model which is more lightweight (smaller size) and possible would scale better for instruct tuning, for our fine tuning task.\n",
        "\n",
        "Good question. And so let me present to you the answer to that question.\n",
        "\n",
        "Let's setup our model for inference"
      ],
      "metadata": {
        "id": "H2KznT7I3v7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle models instances versions download google/gemma-2/transformers/gemma-2-2b-it/2"
      ],
      "metadata": {
        "id": "lxacDXC_XXLZ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!tar -xvzf 'gemma-2.tar.gz' -C 'gemma-2-2b-it'"
      ],
      "metadata": {
        "id": "fZHYGb83wVFF",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "4TUAFZR8tPzm",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"gemma-2-2b-it\")"
      ],
      "metadata": {
        "id": "IZndnU5EtPzn",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"gemma-2-2b-it\",quantization_config=bnb_config,\n",
        "                                                                         device_map='auto')"
      ],
      "metadata": {
        "id": "NtflmkcctPzn",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish and English. You respond to users in a clear, and concise manner in the language of the user query. \\nआप जेम्मा2 हैं, एक मददगार, संवादी एआई सहायक। आप हिंदी, बोलचाल की हिंग्लिश और अंग्रेजी में विशेषज्ञ हैं। आप उपयोगकर्ताओं को उपयोगकर्ता की क्वेरी की भाषा में स्पष्ट और संक्षिप्त तरीके से जवाब देते हैं।\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: Why is diwali celebrated<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = system_prompt + \"\\n\" + user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=2048,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "# Extract model output after <start_of_turn>model: and before <end_of_turn>\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "uU21qAT4kp6F",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "response = ''\n",
        "parts = generated_text.split(\"<start_of_turn>model:\")\n",
        "\n",
        "if len(parts) > 1:\n",
        "    model_response = parts[1]  # This part contains the model's output\n",
        "    model_response = model_response.split(\"<end_of_turn>\")[0].strip()  # Remove after <end_of_turn>\n",
        "    response += model_response\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "mjvwjhv0kY1J",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "The instruct model is already highly capable in generating high quality answers out of the box! And any further fine tuning on it only diminished it's quality rather than improving it. And so we stuck with enhancing the base model for our language task instead.\n",
        "\n",
        "---\n",
        "\n",
        "We Deviced a series of simple questions to test the instruct model"
      ],
      "metadata": {
        "id": "eGyfEBf9r_9g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    {\n",
        "        \"category\": \"General\",\n",
        "        \"user\": \"दुनिया का सबसे ऊँचा पर्वत कौन सा है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"General\",\n",
        "        \"user\": \"पानी का रासायनिक सूत्र क्या है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"General\",\n",
        "        \"user\": \"“सूर्य” शब्द का पर्यायवाची क्या है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"General\",\n",
        "        \"user\": \"पृथ्वी पर सबसे बड़ा महासागर कौन सा है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Chat\",\n",
        "        \"user\": \"तुम कैसे हो?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Chat\",\n",
        "        \"user\": \"क्या तुम मेरे दोस्त बनोगे?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Chat\",\n",
        "        \"user\": \"आज का मौसम कैसा रहेगा?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Chat\",\n",
        "        \"user\": \"मुझे बोरियत हो रही है, क्या कोई मजेदार बात सुनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Historical\",\n",
        "        \"user\": \"महात्मा गांधी का असली नाम क्या था?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Historical\",\n",
        "        \"user\": \"अशोक महान किस राजवंश से संबंधित थे?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Historical\",\n",
        "        \"user\": \"भारत का स्वतंत्रता संग्राम कब शुरू हुआ?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Historical\",\n",
        "        \"user\": \"ताजमहल किसने बनवाया और क्यों?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Storytelling\",\n",
        "        \"user\": \"एक ऐसी कहानी सुनाओ जिसमें राजा, रानी और एक जादुई तोता हो।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Storytelling\",\n",
        "        \"user\": \"किसी बच्चे की साहस की कहानी सुनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Storytelling\",\n",
        "        \"user\": \"चंदामामा की कोई कहानी सुनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Storytelling\",\n",
        "        \"user\": \"मुझे एक रोमांचक जंगल यात्रा की कहानी बताओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Poetry\",\n",
        "        \"user\": \"गुलाब पर एक कविता सुनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Poetry\",\n",
        "        \"user\": \"बारिश के मौसम पर दो लाइनें बनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Poetry\",\n",
        "        \"user\": \"प्रेम पर एक छोटी कविता सुनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Poetry\",\n",
        "        \"user\": \"अपने मन से कोई कविता लिखो।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Hinglish\",\n",
        "        \"user\": \"Tum kya kar rahe ho abhi?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Hinglish\",\n",
        "        \"user\": \"Mujhe ek achhi movie recommend karo.\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Hinglish\",\n",
        "        \"user\": \"Life ke baare mein tumhara kya opinion hai?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Hinglish\",\n",
        "        \"user\": \"Ek short story sunao jo funny ho.\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Knowledge\",\n",
        "        \"user\": \"भारत का राष्ट्रीय पक्षी कौन है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Knowledge\",\n",
        "        \"user\": \"E=mc² का मतलब क्या है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Knowledge\",\n",
        "        \"user\": \"चंद्रग्रहण क्यों और कैसे होता है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Knowledge\",\n",
        "        \"user\": \"विज्ञान के कौन से अविष्कार ने मानव जीवन को सबसे ज्यादा बदला?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Fun\",\n",
        "        \"user\": \"अगर तुम एक जादुई प्राणी होते, तो कौन से होते?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Fun\",\n",
        "        \"user\": \"अपना पसंदीदा खाना बताओ, लेकिन सिर्फ emojis में।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Fun\",\n",
        "        \"user\": \"अगर तुम्हें टाइम मशीन मिल जाए, तो कहां जाना चाहोगे?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Fun\",\n",
        "        \"user\": \"मुझे एक दिन के लिए राजा बना दो, क्या करोगे?\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "K9DznBrtg1DP",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function\n",
        "def generate_responses(dataset, base_model, tokenizer, system_prompt):\n",
        "    \"\"\"\n",
        "    Generate responses for each input in a dataset using a conversational model.\n",
        "\n",
        "    Args:\n",
        "        dataset (list): A list of dictionaries with 'category' and 'user' keys.\n",
        "        base_model (AutoModelForCausalLM): The pre-trained model for generating responses.\n",
        "        tokenizer (AutoTokenizer): The tokenizer for the model.\n",
        "        system_prompt (str): The system prompt to provide context for the model.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated dataset with an additional 'output' field containing the model's response.\n",
        "    \"\"\"\n",
        "    updated_dataset = []\n",
        "\n",
        "    for entry in tqdm(dataset):\n",
        "        user_input = f\"<start_of_turn>user: {entry['user']}<end_of_turn>\"\n",
        "        model_output = \"<start_of_turn>model: \"\n",
        "        combined_input = system_prompt + \"\\n\" + user_input + \"\\n\" + model_output\n",
        "\n",
        "        # Tokenize and prepare input\n",
        "        inputs = tokenizer(combined_input, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "        # Generate response\n",
        "        generated_ids = base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=2048,\n",
        "            do_sample=True,\n",
        "            temperature=1,\n",
        "            top_p=0.95,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.0\n",
        "        )\n",
        "\n",
        "        # Decode the generated output\n",
        "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "\n",
        "        # Extract the actual response by trimming the unnecessary parts\n",
        "        response_text = response.split(\"<start_of_turn>model:\")[1].split(\"<end_of_turn>\")[0].strip()\n",
        "\n",
        "        # Update the entry with the generated output\n",
        "        updated_entry = {\n",
        "            \"category\": entry[\"category\"],\n",
        "            \"user\": entry[\"user\"],\n",
        "            \"output\": response_text\n",
        "        }\n",
        "        updated_dataset.append(updated_entry)\n",
        "\n",
        "    return updated_dataset"
      ],
      "metadata": {
        "id": "5FCtFfMFgIZn",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = (\n",
        "    \"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish and English. \"\n",
        "    \"You respond to users in a clear, and concise manner in the language of the user query. \\n\"\n",
        "    \"आप जेम्मा2 हैं, एक मददगार, संवादी एआई सहायक। आप हिंदी, बोलचाल की हिंग्लिश और अंग्रेजी में विशेषज्ञ हैं। \"\n",
        "    \"आप उपयोगकर्ताओं को उपयोगकर्ता की क्वेरी की भाषा में स्पष्ट और संक्षिप्त तरीके से जवाब देते हैं।\"\n",
        ")\n",
        "# Generate responses\n",
        "updated_dataset = generate_responses(test_prompts, base_model, tokenizer, system_prompt)"
      ],
      "metadata": {
        "id": "R4we3w4vihaJ",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "updated_dataset"
      ],
      "metadata": {
        "id": "8oFuwrafnQ6m",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can see the responses are already very good. Much better than the base model as we will see in a second."
      ],
      "metadata": {
        "id": "Is8dTP_as3J2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "mtBc4r6Fx92T",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "#If you add the model from Kaggle, use this line.\n",
        "modelName = \"/content/gemma-2-2b\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(modelName)\n",
        "base_model = AutoModelForCausalLM.from_pretrained(modelName,\n",
        "                                             quantization_config=bnb_config,\n",
        "                                             trust_remote_code=True,\n",
        "                                             device_map=\"auto\")"
      ],
      "metadata": {
        "id": "LELZQ5x4Nn8G",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish and English. You respond to users in a clear, and concise manner in the language of the user query. \\nआप जेम्मा2 हैं, एक मददगार, संवादी एआई सहायक। आप हिंदी, बोलचाल की हिंग्लिश और अंग्रेजी में विशेषज्ञ हैं। आप उपयोगकर्ताओं को उपयोगकर्ता की क्वेरी की भाषा में स्पष्ट और संक्षिप्त तरीके से जवाब देते हैं।\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: Why is diwali celebrated<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = system_prompt + \"\\n\" + user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=2048,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0])"
      ],
      "metadata": {
        "id": "byszk53Rx92U",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can already see the responses are very bad"
      ],
      "metadata": {
        "id": "AcRKfIzKyAPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = (\n",
        "    \"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish and English. \"\n",
        "    \"You respond to users in a clear, and concise manner in the language of the user query. \\n\"\n",
        "    \"आप जेम्मा2 हैं, एक मददगार, संवादी एआई सहायक। आप हिंदी, बोलचाल की हिंग्लिश और अंग्रेजी में विशेषज्ञ हैं। \"\n",
        "    \"आप उपयोगकर्ताओं को उपयोगकर्ता की क्वेरी की भाषा में स्पष्ट और संक्षिप्त तरीके से जवाब देते हैं।\"\n",
        ")\n",
        "# Generate responses\n",
        "updated_dataset = generate_responses(test_prompts, base_model, tokenizer, system_prompt)"
      ],
      "metadata": {
        "id": "eNOcRh5_yKFA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "updated_dataset"
      ],
      "metadata": {
        "id": "bbBjLnAJyKFA",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can compare these base responses with the instruct model. They are worlds apart.\n",
        "\n",
        "Due to this difference we decided to fine tune the base model as that would be more suitable as per the goal of our project"
      ],
      "metadata": {
        "id": "bggkhQ4LyPG2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Final testing"
      ],
      "metadata": {
        "id": "nMH6TfW4JfGi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are gonna test our model for QnA and RAG along with some Few-Shotting for improving the results"
      ],
      "metadata": {
        "id": "qqgw1Fg6cxcO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### QnA Testing"
      ],
      "metadata": {
        "id": "5XzpfUSaPJhY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let us see how does our model performs on the same series of test questions"
      ],
      "metadata": {
        "id": "siBhCuasJ0FH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")"
      ],
      "metadata": {
        "id": "98BI4DD6KB5X",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/MyDrive/Bhavesh/Models/google/gemma-2-2b-(hi)-base+wiki+alpaca+databrick+mathquest\")"
      ],
      "metadata": {
        "id": "swA6CNUcKB5X",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForCausalLM.from_pretrained(\"/content/drive/MyDrive/Bhavesh/Models/google/gemma-2-2b-(hi)-base+wiki+alpaca+databrick+mathquest\",quantization_config=bnb_config,\n",
        "                                                                         device_map='auto')"
      ],
      "metadata": {
        "id": "6zb0FCqGKB5X",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompts = [\n",
        "    {\n",
        "        \"category\": \"General\",\n",
        "        \"user\": \"दुनिया का सबसे ऊँचा पर्वत कौन सा है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"General\",\n",
        "        \"user\": \"पानी का रासायनिक सूत्र क्या है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"General\",\n",
        "        \"user\": \"“सूर्य” शब्द का पर्यायवाची क्या है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"General\",\n",
        "        \"user\": \"पृथ्वी पर सबसे बड़ा महासागर कौन सा है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Chat\",\n",
        "        \"user\": \"तुम कैसे हो?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Chat\",\n",
        "        \"user\": \"क्या तुम मेरे दोस्त बनोगे?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Chat\",\n",
        "        \"user\": \"आज का मौसम कैसा रहेगा?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Chat\",\n",
        "        \"user\": \"मुझे बोरियत हो रही है, क्या कोई मजेदार बात सुनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Historical\",\n",
        "        \"user\": \"महात्मा गांधी का असली नाम क्या था?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Historical\",\n",
        "        \"user\": \"अशोक महान किस राजवंश से संबंधित थे?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Historical\",\n",
        "        \"user\": \"भारत का स्वतंत्रता संग्राम कब शुरू हुआ?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Historical\",\n",
        "        \"user\": \"ताजमहल किसने बनवाया और क्यों?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Storytelling\",\n",
        "        \"user\": \"एक ऐसी कहानी सुनाओ जिसमें राजा, रानी और एक जादुई तोता हो।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Storytelling\",\n",
        "        \"user\": \"किसी बच्चे की साहस की कहानी सुनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Storytelling\",\n",
        "        \"user\": \"चंदामामा की कोई कहानी सुनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Storytelling\",\n",
        "        \"user\": \"मुझे एक रोमांचक जंगल यात्रा की कहानी बताओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Poetry\",\n",
        "        \"user\": \"गुलाब पर एक कविता सुनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Poetry\",\n",
        "        \"user\": \"बारिश के मौसम पर दो लाइनें बनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Poetry\",\n",
        "        \"user\": \"प्रेम पर एक छोटी कविता सुनाओ।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Poetry\",\n",
        "        \"user\": \"अपने मन से कोई कविता लिखो।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Hinglish\",\n",
        "        \"user\": \"Tum kya kar rahe ho abhi?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Hinglish\",\n",
        "        \"user\": \"Mujhe ek achhi movie recommend karo.\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Hinglish\",\n",
        "        \"user\": \"Life ke baare mein tumhara kya opinion hai?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Hinglish\",\n",
        "        \"user\": \"Ek short story sunao jo funny ho.\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Knowledge\",\n",
        "        \"user\": \"भारत का राष्ट्रीय पक्षी कौन है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Knowledge\",\n",
        "        \"user\": \"E=mc² का मतलब क्या है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Knowledge\",\n",
        "        \"user\": \"चंद्रग्रहण क्यों और कैसे होता है?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Knowledge\",\n",
        "        \"user\": \"विज्ञान के कौन से अविष्कार ने मानव जीवन को सबसे ज्यादा बदला?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Fun\",\n",
        "        \"user\": \"अगर तुम एक जादुई प्राणी होते, तो कौन से होते?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Fun\",\n",
        "        \"user\": \"अपना पसंदीदा खाना बताओ, लेकिन सिर्फ emojis में।\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Fun\",\n",
        "        \"user\": \"अगर तुम्हें टाइम मशीन मिल जाए, तो कहां जाना चाहोगे?\"\n",
        "    },\n",
        "    {\n",
        "        \"category\": \"Fun\",\n",
        "        \"user\": \"मुझे एक दिन के लिए राजा बना दो, क्या करोगे?\"\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "mwDrSPKyKB5Y",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function\n",
        "def generate_responses(dataset, base_model, tokenizer, system_prompt=''):\n",
        "    \"\"\"\n",
        "    Generate responses for each input in a dataset using a conversational model.\n",
        "\n",
        "    Args:\n",
        "        dataset (list): A list of dictionaries with 'category' and 'user' keys.\n",
        "        base_model (AutoModelForCausalLM): The pre-trained model for generating responses.\n",
        "        tokenizer (AutoTokenizer): The tokenizer for the model.\n",
        "        system_prompt (str): The system prompt to provide context for the model.\n",
        "\n",
        "    Returns:\n",
        "        list: Updated dataset with an additional 'output' field containing the model's response.\n",
        "    \"\"\"\n",
        "    updated_dataset = []\n",
        "\n",
        "    for entry in tqdm(dataset):\n",
        "        user_input = f\"<start_of_turn>user: {entry['user']}<end_of_turn>\"\n",
        "        model_output = \"<start_of_turn>model: \"\n",
        "        combined_input = system_prompt + \"\\n\" + user_input + \"\\n\" + model_output\n",
        "\n",
        "        # Tokenize and prepare input\n",
        "        inputs = tokenizer(combined_input, return_tensors=\"pt\").to('cuda')\n",
        "\n",
        "        # Generate response\n",
        "        generated_ids = base_model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=2048,\n",
        "            do_sample=True,\n",
        "            temperature=1,\n",
        "            top_p=0.95,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.0\n",
        "        )\n",
        "\n",
        "        # Decode the generated output\n",
        "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=False)[0]\n",
        "\n",
        "        # Extract the actual response by trimming the unnecessary parts\n",
        "        response_text = response.split(\"<start_of_turn>model:\")[1].split(\"<end_of_turn>\")[0].strip()\n",
        "\n",
        "        # Update the entry with the generated output\n",
        "        updated_entry = {\n",
        "            \"category\": entry[\"category\"],\n",
        "            \"user\": entry[\"user\"],\n",
        "            \"output\": response_text\n",
        "        }\n",
        "        updated_dataset.append(updated_entry)\n",
        "\n",
        "    return updated_dataset"
      ],
      "metadata": {
        "id": "r-WODxaDKB5Y",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = (\n",
        "    \"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish and English. \"\n",
        "    \"You respond to users in a clear, and concise manner in the language of the user query. \\n\"\n",
        "    \"आप जेम्मा2 हैं, एक मददगार, संवादी एआई सहायक। आप हिंदी, बोलचाल की हिंग्लिश और अंग्रेजी में विशेषज्ञ हैं। \"\n",
        "    \"आप उपयोगकर्ताओं को उपयोगकर्ता की क्वेरी की भाषा में स्पष्ट और संक्षिप्त तरीके से जवाब देते हैं।\"\n",
        ")\n",
        "# Generate responses\n",
        "updated_dataset = generate_responses(test_prompts, base_model, tokenizer, system_prompt)"
      ],
      "metadata": {
        "id": "mStJ41_wKB5Y",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "updated_dataset"
      ],
      "metadata": {
        "id": "sLgZFGgqKB5Y",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RAG Testing Along with Few-Shot Prompting"
      ],
      "metadata": {
        "id": "P1Fo6TwpPPxC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We tested it in Historical aspect"
      ],
      "metadata": {
        "id": "40fL-nx0P_vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are Gemma2, a helpful, conversational AI assistant integrated with a Retrieval-Augmented Generation (RAG) system.\n",
        "You are an expert in Hindi, colloquial Hinglish, and English. When responding to user queries, you:\n",
        "- Retrieve relevant information from the integrated knowledge base or external sources when needed.\n",
        "- Provide clear, concise, and accurate responses in the language of the user query.\"\"\"\n",
        "\n",
        "retrieved_info = \"\"\"Retrieved information:\n",
        "- Diwali is celebrated to commemorate the return of Lord Rama to Ayodhya after a 14-year exile, during which he defeated Ravana.\n",
        "- It symbolizes the victory of light over darkness and good over evil.\n",
        "- Source: Indian Mythology Knowledge Base\"\"\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: Why is diwali celebrated<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = system_prompt + \"\\n\" +user_input + \"\\n\" + retrieved_info + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=500,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "4KpqwhzYOgno",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish, and English. When responding to user queries, you'll provide clear, concise, and accurate responses based on \"Retrieved Information\" in the language of the user query.\"\"\"\n",
        "\n",
        "retrieved_info = \"\"\"Retrieved information:\n",
        "- Diwali is celebrated to commemorate the return of Lord Rama to Ayodhya after a 14-year exile, during which he defeated Ravana.\n",
        "- It symbolizes the victory of light over darkness and good over evil.\"\"\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: Why is diwali celebrated<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = system_prompt + \"\\n\" +user_input + \"\\n\" + retrieved_info + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=500,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "ciRWXh-6tF5I",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish, and English. When responding to user queries, you'll provide clear, concise, and accurate responses based on \"Retrieved Information\" in the language of the user query. \\n आप Gemma2 हैं, एक सहायक, बातचीत करने वाली AI सहायक। आप हिंदी, आम बोलचाल की हिंग्लिश और अंग्रेज़ी में विशेषज्ञ हैं। उपयोगकर्ता की क्वेरी का उत्तर 'Retrieved Information' के आधार पर स्पष्ट, संक्षिप्त और उपयोगकर्ता की क्वेरी की भाषा में दें।\"\"\"\n",
        "\n",
        "retrieved_info = \"\"\"Retrieved information:\n",
        "- Diwali is celebrated to commemorate the return of Lord Rama to Ayodhya after a 14-year exile, during which he defeated Ravana.\n",
        "- It symbolizes the victory of light over darkness and good over evil.\"\"\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"<start_of_turn>user: Why is diwali celebrated<end_of_turn>\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = system_prompt + \"\\n\" +user_input + \"\\n\" + retrieved_info + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=500,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "rVCVIeJ5pkgB",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: As you can see the RAG Response in English is pretty accurate and true to the context along with some extra information\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "EzAzk7QkQMuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"You are Gemma2, a helpful, conversational AI assistant. You are an expert in Hindi, colloquial Hinglish, and English. Answer the user in clear concise and manner in the language of the user query. You will answer the user question based on the information only\"\"\"\n",
        "\n",
        "# Prepare the input\n",
        "user_input = \"\"\"<start_of_turn>user: दीवाली क्यों मनाई जाती है? Answer - \"दीवाली मनाई जाती है भगवान राम की अयोध्या वापसी की स्मृति में, जो 14 वर्षों के वनवास के बाद हुई, इस दौरान उन्होंने रावण का वध किया। यह अंधकार पर प्रकाश और बुराई पर अच्छाई की विजय का प्रतीक है। स्रोत: भारतीय पौराणिक ज्ञान आधार\" <end_of_turn>\"\"\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "combined_input = system_prompt + \"\\n\" +user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=2048,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.0)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "oSAwr-XmtF5K",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\" You will answer \"user\" query based on the information only. \\nInformation - \"दीवाली मनाई जाती है भगवान राम की अयोध्या वापसी की स्मृति में, जो 14 वर्षों के वनवास के बाद हुई, इस दौरान उन्होंने रावण का वध किया। यह अंधकार पर प्रकाश और बुराई पर अच्छाई की विजय का प्रतीक है। स्रोत: भारतीय पौराणिक ज्ञान आधार\" \"\"\"\n",
        "\n",
        "# Input Preparation\n",
        "user_input = \"\"\"<start_of_turn>user: \"Diwali kyu manai jaati hay?\"<end_of_turn>\"\"\"\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "\n",
        "# Combine Input for RAG\n",
        "combined_input = system_prompt + \"\\n\" + user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=2048,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1.5)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "a3T744L8lHzB",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hindi and code-mix or Hinglish Responses were slightly true to the context however weren't too accurate and additional, sometimes good sometimes garbbled but close responses were being generated however not aligning with the Retrieved Information\n",
        "\n",
        "---\n",
        "\n",
        "We tried many keywords and prompts and finally settles with a Few-Shot attempt and to provide the retrieved information within the `<end_of_turn>` token\n",
        "\n",
        "It is also advisable to keep the temperature low while RAG application"
      ],
      "metadata": {
        "id": "5DrzDOuqRdUa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are Gemma2, a helpful, conversational AI assistant with Retrieval-Augmented Generation capabilities.\n",
        "You are an expert in Hindi, colloquial Hinglish, and English. Respond to the user in a clear, concise manner in the language of the query.\n",
        "Always base your answers solely on the 'Retrieved Information.' Avoid producing unnecessary output or adding extra context.\n",
        "\n",
        "Analyze these Examples:\n",
        "\n",
        "Example 1: Hindi\n",
        "User: \"चंद्रग्रहण क्या है?\"\n",
        "Retrieved Information: 'चंद्रग्रहण तब होता है जब चंद्रमा पृथ्वी की छाया में प्रवेश करता है। यह पूर्ण और आंशिक हो सकता है। स्रोत: खगोल विज्ञान ज्ञान आधार'\n",
        "Model: \"चंद्रग्रहण तब होता है जब चंद्रमा पृथ्वी की छाया में आता है।\"\n",
        "\n",
        "Example 2: Hinglish\n",
        "User: \"What is the meaning of aurora borealis?\"\n",
        "Retrieved Information: 'Aurora Borealis, also known as the Northern Lights, is a natural light display in Earth's sky, predominantly seen in high-latitude regions. Source: Encyclopedia of Natural Phenomena'\n",
        "Model: \"Aurora Borealis is the Northern Lights seen in high-latitude regions.\"\n",
        "\n",
        "Example 3: English\n",
        "User: \"What is the capital of France?\"\n",
        "Retrieved Information: 'The capital of France is Paris. Source: World Geography Database'\n",
        "Model: \"The capital of France is Paris.\"\n",
        "\n",
        "Example 4: Hinglish\n",
        "User: \"Volcano kya hota hai?\"\n",
        "Retrieved Information: 'A volcano is an opening in Earth's surface where molten rock, ash, and gases erupt. It forms mountains over time. Source: Geological Facts'\n",
        "Model: \"Volcano ek opening hai jahan se molten rock aur gases erupt karte hain.\"\n",
        "\n",
        "Example 5: Hindi\n",
        "User: \"भारत का राष्ट्रीय पक्षी कौन सा है?\"\n",
        "Retrieved Information: 'भारत का राष्ट्रीय पक्षी मोर है। स्रोत: भारतीय ज्ञान कोश'\n",
        "Model: \"भारत का राष्ट्रीय पक्षी मोर है।\"\n",
        "\n",
        "Now answer the user question based on the 'Retrieved Information' only.\n",
        "\"\"\"\n",
        "\n",
        "# Retrieval-Augmented Input\n",
        "rag = \"\"\"Retrieved Information - 'दीवाली मनाई जाती है भगवान राम की अयोध्या वापसी की स्मृति में, जो 14 वर्षों के वनवास के बाद हुई, इस दौरान उन्होंने रावण का वध किया। यह अंधकार पर प्रकाश और बुराई पर अच्छाई की विजय का प्रतीक है। स्रोत: भारतीय पौराणिक ज्ञान आधार'\"\"\"\n",
        "\n",
        "# User Input\n",
        "user_input = f\"\"\"<start_of_turn>user: Answer in short - \"दीवाली क्यों मनाई जाती है?\" \\n{rag} \\n<end_of_turn>\"\"\"\n",
        "\n",
        "# Model Output Placeholder\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "\n",
        "# Combine Input for RAG\n",
        "combined_input = system_prompt + \"\\n\" + user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=500,\n",
        "                              do_sample=True,\n",
        "                              temperature=1,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "qqgIoF26nFFi",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "You can see the difference when we reduce the temperature"
      ],
      "metadata": {
        "id": "3RbI5bBH7Ld9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"\"\"\n",
        "You are Gemma2, a helpful, conversational AI assistant with Retrieval-Augmented Generation capabilities.\n",
        "You are an expert in Hindi, colloquial Hinglish, and English. Respond to the user in a clear, concise manner in the language of the query.\n",
        "Always base your answers solely on the 'Retrieved Information.' Avoid producing unnecessary output or adding extra context.\n",
        "\n",
        "Analyze these Examples:\n",
        "\n",
        "Example 1: Hindi\n",
        "User: \"चंद्रग्रहण क्या है?\"\n",
        "Retrieved Information: 'चंद्रग्रहण तब होता है जब चंद्रमा पृथ्वी की छाया में प्रवेश करता है। यह पूर्ण और आंशिक हो सकता है। स्रोत: खगोल विज्ञान ज्ञान आधार'\n",
        "Model: \"चंद्रग्रहण तब होता है जब चंद्रमा पृथ्वी की छाया में आता है।\"\n",
        "\n",
        "Example 2: Hinglish\n",
        "User: \"What is the meaning of aurora borealis?\"\n",
        "Retrieved Information: 'Aurora Borealis, also known as the Northern Lights, is a natural light display in Earth's sky, predominantly seen in high-latitude regions. Source: Encyclopedia of Natural Phenomena'\n",
        "Model: \"Aurora Borealis is the Northern Lights seen in high-latitude regions.\"\n",
        "\n",
        "Example 3: English\n",
        "User: \"What is the capital of France?\"\n",
        "Retrieved Information: 'The capital of France is Paris. Source: World Geography Database'\n",
        "Model: \"The capital of France is Paris.\"\n",
        "\n",
        "Now answer the user question based on the 'Retrieved Information' only.\n",
        "\"\"\"\n",
        "\n",
        "# Retrieval-Augmented Input\n",
        "rag = \"\"\"Retrieved Information - 'दीवाली मनाई जाती है भगवान राम की अयोध्या वापसी की स्मृति में, जो 14 वर्षों के वनवास के बाद हुई, इस दौरान उन्होंने रावण का वध किया। यह अंधकार पर प्रकाश और बुराई पर अच्छाई की विजय का प्रतीक है। स्रोत: भारतीय पौराणिक ज्ञान आधार'\"\"\"\n",
        "\n",
        "# User Input\n",
        "user_input = f\"\"\"<start_of_turn>user: Answer in short - \"दीवाली क्यों मनाई जाती है?\" \\n{rag} \\n<end_of_turn>\"\"\"\n",
        "\n",
        "# Model Output Placeholder\n",
        "model_output = \"<start_of_turn>model: \"\n",
        "\n",
        "# Combine Input for RAG\n",
        "combined_input = system_prompt + \"\\n\" + user_input + \"\\n\" + model_output\n",
        "\n",
        "\n",
        "inputs = tokenizer(combined_input, return_tensors=\"pt\").to(base_model.device)\n",
        "\n",
        "generated_ids = base_model.generate(**inputs,\n",
        "                              max_new_tokens=500,\n",
        "                              do_sample=True,\n",
        "                              temperature=0.5,\n",
        "                              top_p=0.95,\n",
        "                              top_k=50,\n",
        "                              repetition_penalty=1)\n",
        "\n",
        "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
      ],
      "metadata": {
        "id": "Ncf-D9w3lqNj",
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "> NOTE: As per these examples, using Few-Shot prompting with lower temperature for RAG instantly improved the model's output making it closer to the Retrieved Information.\n",
        "\n",
        "- This wasn't needed during English RAG although for Hindi it seems to help the model understand how to use the Information provided to it, much efficiently\n",
        "\n",
        "- Training the model even longer on broader knowledge base and QnA datasets can vastly improve the results specially considering how quickly it learnt from a very small amount of examples in our last Training\n",
        "---"
      ],
      "metadata": {
        "id": "h_Pq0EvsP9dJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Conclusion and Findings"
      ],
      "metadata": {
        "id": "kgUuVCakOzwS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Our Findings**\n",
        "\n",
        "- As a native hindi speaker I can say that although it has learnt to answer in hindi now, the answers are not accurate at all. Except may be a few.\n",
        "\n",
        "- However this is still a drastic difference and change from the original base model.\n",
        "\n",
        "- This approach of combining several Wikipedia articles with titles as user query and text as model output has proven effective.\n",
        "\n",
        "- Although the given answers are wrong the hallucination upon giving a Hindi prompt has gone down drastically.\n",
        "\n",
        "- The model did learn to follow the system prompt and output in the language of the given query if given the right system prompt.\n",
        "\n",
        "- The model's performance and answer accuracy can be further improved by giving it much rich documents about several topics such as Chemistery, History, Geography etc\n",
        "\n",
        "- Including chat conversations along with knowledge injection has also shown improvements in a chat like response generation.\n",
        "\n",
        "- One thing to note is that even though we trained our model for **40 Hrs**, this is a sum total time on all three trainings. However our most preferred one was the last attempt of training on mixed text corpus which had given us best result so far. And in that attempt we only trained for **4 Hrs** on a small subset of dataset due to resource and time constraint. This\n",
        "goes to show us that if we had trained on that mixed corpus for longer the result would have been exponentially better\n",
        "\n",
        "- Further more subsequent runs on same prompt can result a better response, overall making it a viable option of getting adopted as a multi-run option or majority voting.\n",
        "\n",
        "**In conclusion replicating our Final Approach can significantly increase the model's performance in learning and answering prompts/chats/queries a new language**"
      ],
      "metadata": {
        "id": "BaE4Y_NdO2R0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Learnings\n",
        "\n"
      ],
      "metadata": {
        "id": "kxplk6v65rW0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Understanding LoRA**\n",
        "\n",
        "LoRA (Low-Rank Adaptation) is a technique to fine-tune large language models efficiently by adapting a small subset of parameters. Here's an explanation of the key parameters and their implications:\n",
        "\n",
        "---\n",
        "\n",
        "**1. Parameters of LoRA**\n",
        "\n",
        "**`r` (Rank)**\n",
        "- **Definition**: The rank of the low-rank decomposition matrix used for parameter adaptation. Lower `r` values mean fewer trainable parameters, making the adaptation more memory-efficient but less expressive.\n",
        "- **High `r`**: Use when the task requires injecting substantial new knowledge or adapting to a domain that is significantly different from the pre-trained model's domain.\n",
        "- **Low `r`**: Use when the task involves subtle adaptations or pattern fine-tuning within a domain close to the pre-trained model's scope.\n",
        "\n",
        "---\n",
        "\n",
        "**`lora_alpha`**\n",
        "- **Definition**: A scaling factor that controls the impact of the LoRA layers on the model.\n",
        "- **High `lora_alpha`**: Amplifies the contribution of the LoRA layers. Useful when large-scale domain shifts or high-impact adaptations are required.\n",
        "- **Low `lora_alpha`**: Reduces the LoRA layers' influence, ensuring minimal disturbance to the pre-trained parameters. Suitable for fine-tuning in similar domains or tasks requiring subtle behavior changes.\n",
        "\n",
        "---\n",
        "\n",
        "**`use_rslora` (Residual LoRA)**\n",
        "- **Definition**: A variant of LoRA that retains residual connections, helping to stabilize training and improve performance in some scenarios.\n",
        "- **When to use**: For tasks with limited data or where maintaining robustness is critical. It reduces the risk of catastrophic forgetting and overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "**2. Target Modules**\n",
        "\n",
        "**What are target modules?**\n",
        "- These are the parts of the model where LoRA applies low-rank updates. Common target modules include:\n",
        "  - `q_proj`: Query projections (attention heads).\n",
        "  - `k_proj`: Key projections.\n",
        "  - `v_proj`: Value projections.\n",
        "  - `o_proj`: Output projections.\n",
        "  - `gate_proj`, `up_proj`, `down_proj`: Parts of feed-forward networks.\n",
        "\n",
        "**Effect of including specific target modules**:\n",
        "- **Attention modules (`q_proj`, `k_proj`, `v_proj`, `o_proj`)**:\n",
        "  - **Focus**: LoRA changes how the model attends to information.\n",
        "  - **When to use**: If the task requires significant changes in how the model interprets input relationships or context.\n",
        "\n",
        "- **Feed-forward network modules (`gate_proj`, `up_proj`, `down_proj`)**:\n",
        "  - **Focus**: LoRA adjusts the transformation and interpretation of features.\n",
        "  - **When to use**: If the task relies on complex transformations or domain-specific feature engineering.\n",
        "\n",
        "- **Broader module inclusion**: Increases the model's ability to adapt but requires more memory and computational resources. It may also risk overfitting if the dataset is small.\n",
        "\n",
        "**Excluding target modules**:\n",
        "- Limits the scope of adaptation, preserving more of the pre-trained knowledge. This can be ideal for fine-tuning on tasks requiring minimal domain shifts.\n",
        "\n",
        "---\n",
        "\n",
        "**3. Modules to Save**\n",
        "- **Definition**: These modules are saved along with LoRA parameters, ensuring the model's modified state is preserved for deployment.\n",
        "- **Impact**:\n",
        "  - Saving modules like `embed_tokens` and `lm_head` ensures that task-specific embeddings or outputs are retained.\n",
        "  - Including broader modules increases the ability to deploy the model for specific tasks but requires careful consideration to avoid saving unnecessary changes.\n",
        "\n",
        "---\n",
        "\n",
        "**4. Choosing Parameter Values**\n",
        "\n",
        "**For Knowledge Injection**:\n",
        "- **Purpose**: Add domain-specific knowledge or train the model for a substantially new task.\n",
        "- **Recommended Settings**:\n",
        "  - **`r`**: Higher (e.g., 32–64) to increase flexibility.\n",
        "  - **`lora_alpha`**: Higher (e.g., 128–256) for stronger influence.\n",
        "  - **Target Modules**: Include a wide range, such as all attention and feed-forward modules.\n",
        "  - **Modules to Save**: Save embeddings, heads, and any adapted layers.\n",
        "\n",
        "**For Pattern Fine-Tuning**:\n",
        "- **Purpose**: Adjust the model for small-scale adaptations or subtle domain shifts.\n",
        "- **Recommended Settings**:\n",
        "  - **`r`**: Lower (e.g., 4–16) for efficiency.\n",
        "  - **`lora_alpha`**: Lower (e.g., 16–64) to ensure subtle updates.\n",
        "  - **Target Modules**: Focus on essential modules like `q_proj`, `v_proj`, and `o_proj`.\n",
        "  - **Modules to Save**: Minimal, often just embeddings or heads.\n",
        "\n",
        "---\n",
        "\n",
        "**5. Practical Considerations**\n",
        "- **Dataset Size**:\n",
        "  - Small datasets benefit from fewer target modules and lower `r` to avoid overfitting.\n",
        "  - Large datasets can leverage higher `r` and broader target modules for richer adaptation.\n",
        "  \n",
        "- **Task Complexity**:\n",
        "  - Complex tasks or significant domain shifts require higher `r` and broader module inclusion.\n",
        "  - Simple tasks or minor shifts work well with limited adaptations.\n",
        "\n",
        "By carefully tuning these parameters based on the task and dataset, you can achieve efficient and effective model fine-tuning using LoRA."
      ],
      "metadata": {
        "id": "-a2z-j1z53lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Summary\n",
        "\n",
        "This project has been a great learning milestone for us. We overcame tons of problems, errors, misbehaviours, dataset curation, model parameters, fine tuning, LLM Training and a lot more.\n",
        "\n",
        "It was a result of 1 week of meticulous experimentaion, research and learning many things from scratch specially for an efficient training.\n",
        "\n",
        "We sincerely Thank Google for this opportunity!\n",
        "\n",
        "If we managed to increase your knowledge base, please give us an upvote. It has taken us, a lot of efforts, trials and errors to get this far."
      ],
      "metadata": {
        "id": "X6RuRTKZ9ETR"
      }
    }
  ]
}