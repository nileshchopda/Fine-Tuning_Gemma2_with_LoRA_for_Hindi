{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":85416,"databundleVersionId":9690815,"sourceType":"competition"},{"sourceId":104623,"sourceType":"modelInstanceVersion","modelInstanceId":72254,"modelId":76277}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Fine-Tuning Gemma 2 with LoRA for Hindi**\n\n# Introduction\n\nLanguage models like Gemma 2 are transformative tools in the world of natural language processing (NLP), enabling applications ranging from text generation to language translation and sentiment analysis. However, these models are often trained on predominantly English datasets, which can lead to underrepresentation of other languages and cultural nuances.\n\nIn this notebook, I aim to address this gap by fine-tuning Gemma 2 for Hindi, a language spoken by over 600 million people worldwide and rich in cultural and linguistic diversity. By fine-tuning the model for Hindi, we empower communities to access NLP technologies tailored to their language, opening doors to enhanced communication, learning, and innovation.\n\nThis notebook is designed with clarity and replicability in mind, ensuring that anyone, regardless of their expertise, can follow along and adapt the process for their own language or context.","metadata":{}},{"cell_type":"markdown","source":"# Setup and Initialization\n\nIn this section, we prepare the environment by installing the necessary libraries and importing essential modules to fine-tune Gemma 2 for Hindi. This step ensures that all dependencies are correctly installed, and the environment is configured for seamless execution of the notebook.\n\n**1. Install Dependencies**\n\n* We install packages such as **transformers**, **datasets**, **accelerate**, **peft**, and others that are essential for working with large language models and fine-tuning tasks.\n* The **bitsandbytes** library is used to enable memory-efficient computations, crucial for handling large models.\n* We also set up **wandb** (Weights & Biases) for experiment tracking and logging.","metadata":{}},{"cell_type":"code","source":"%%capture\n%pip install -U transformers datasets accelerate peft trl bitsandbytes","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:49:39.565150Z","iopub.execute_input":"2025-02-28T05:49:39.566377Z","iopub.status.idle":"2025-02-28T05:49:59.990264Z","shell.execute_reply.started":"2025-02-28T05:49:39.566346Z","shell.execute_reply":"2025-02-28T05:49:59.989002Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"**2. Import Libraries**\n* Key libraries for loading and fine-tuning the model (**transformers**, **peft**, **trl**) are imported.\n* Supporting libraries like **torch**, **wandb**, and **datasets** are also loaded to streamline model training and data handling.","metadata":{}},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\n\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\n\nimport os\nimport torch\nimport bitsandbytes as bnb\n\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, SFTConfig, setup_chat_format","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:50:23.241028Z","iopub.execute_input":"2025-02-28T05:50:23.241317Z","iopub.status.idle":"2025-02-28T05:50:39.675679Z","shell.execute_reply.started":"2025-02-28T05:50:23.241296Z","shell.execute_reply":"2025-02-28T05:50:39.674797Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"**3. Define Base Variables**\n* We define the base model (**google/gemma-2-2b-it**), the dataset to be used for fine-tuning (**maharnab/hindi_instruct**), and the name of the new fine-tuned model (**Gemma-2-2b-it-hindi**).","metadata":{}},{"cell_type":"code","source":"base_model = \"/kaggle/input/gemma-2/transformers/gemma-2-2b-it/2/\"\ndataset_name = \"maharnab/hindi_instruct\"\nnew_model = \"Gemma-2-2b-it-hindi\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:50:39.676772Z","iopub.execute_input":"2025-02-28T05:50:39.677059Z","iopub.status.idle":"2025-02-28T05:50:39.680732Z","shell.execute_reply.started":"2025-02-28T05:50:39.677029Z","shell.execute_reply":"2025-02-28T05:50:39.679935Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"# Hardware Setup and Model Configuration\n\nIn this section, we ensure that the hardware and model configurations are optimized for fine-tuning Gemma 2 using techniques like Quantized LoRA (QLoRA) and Flash Attention.","metadata":{}},{"cell_type":"markdown","source":"**1. Check CUDA Device Capability**\n\n* The CUDA device capability is checked to determine whether the hardware supports advanced features like Flash Attention v2\n* If it meets the capability, Install and configure Flash Attention v2 for performance-boosting memory and speed enhancements and Set the appropriate data type for computations (bfloat16 for newer GPUs, float16 for older ones).","metadata":{}},{"cell_type":"code","source":"# Check CUDA device capability and set appropriate configurations\n# Flash Attention v2 requires CUDA device capability >= 8.0\nif torch.cuda.get_device_capability()[0] >= 8:\n    # Install Flash Attention if capability allows\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16               # Use bfloat16 precision for better performance on supported hardware\n    attn_implementation = \"flash_attention_2\"  # Use Flash Attention v2\nelse:\n    torch_dtype = torch.float16    # Use float16 for older hardware\n    attn_implementation = \"eager\"  # Default attention implementation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:51:22.162810Z","iopub.execute_input":"2025-02-28T05:51:22.163118Z","iopub.status.idle":"2025-02-28T05:51:22.167945Z","shell.execute_reply.started":"2025-02-28T05:51:22.163093Z","shell.execute_reply":"2025-02-28T05:51:22.166990Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"**2. Configure Quantized LoRA (QLoRA)**\n\n* We use **BitsAndBytesConfig** to enable 4-bit quantization, which allows efficient loading of large models without compromising much on performance.\n* Additional settings like **nf4** quantization type and double quantization ensure improved model precision and computational efficiency.","metadata":{}},{"cell_type":"code","source":"# Configuration for Quantized LoRA (QLoRA)\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,                   # Enable 4-bit quantization for efficient model loading\n    bnb_4bit_quant_type=\"nf4\",           # Use NormalFloat4 (NF4) quantization\n    bnb_4bit_compute_dtype=torch_dtype,  # Set computation precision based on hardware support\n    bnb_4bit_use_double_quant=True       # Use double quantization for improved accuracy\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:51:41.378919Z","iopub.execute_input":"2025-02-28T05:51:41.379202Z","iopub.status.idle":"2025-02-28T05:51:41.384474Z","shell.execute_reply.started":"2025-02-28T05:51:41.379182Z","shell.execute_reply":"2025-02-28T05:51:41.383571Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"**3. Load Pretrained Model and Tokenizer**\n\n* The causal language model (**Gemma 2**) is loaded with the specified quantization and attention configurations.\n* The tokenizer corresponding to the base model is loaded, ensuring compatibility with the fine-tuning process.","metadata":{}},{"cell_type":"code","source":"# Load the pretrained causal language model with quantization configuration\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,                              # The base model identifier or path\n    quantization_config=quantization_config,          # Apply QLoRA configuration\n    device_map=\"auto\",                       # Automatically map model to available devices\n    attn_implementation=attn_implementation  # Set attention implementation\n)\n\n# Load the tokenizer corresponding to the pretrained model\ntokenizer = AutoTokenizer.from_pretrained(\n    base_model,             # The base model identifier or path\n    trust_remote_code=True  # Trust custom tokenizer code if provided by the model\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:52:03.535164Z","iopub.execute_input":"2025-02-28T05:52:03.535577Z","iopub.status.idle":"2025-02-28T05:52:35.142790Z","shell.execute_reply.started":"2025-02-28T05:52:03.535543Z","shell.execute_reply":"2025-02-28T05:52:35.141905Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c313a91e1d4642019c7b59f59845b56e"}},"metadata":{}}],"execution_count":6},{"cell_type":"markdown","source":"# Configuring LoRA for Fine-Tuning\n\n**1. Identify Linear Layers**\n\n* Define a helper function to locate all 4-bit linear layers in the model, which are suitable for LoRA fine-tuning.\n* Exclude the **lm_head** layer to focus on trainable components.","metadata":{}},{"cell_type":"code","source":"def find_all_linear_names(model):\n    \"\"\"\n    This function searches for all linear layers of the 4-bit format \n    in a given model and returns their names, excluding the 'lm_head' \n    module if present.\n\n    Args:\n    - model: The model to search for linear layers in.\n\n    Returns:\n    - List of module names associated with linear layers.\n    \"\"\"\n    # The target class for linear layers (4-bit format)\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()  # Set to hold the unique names of the target linear modules\n\n    # Iterate over all named modules in the model\n    for name, module in model.named_modules():\n        # Check if the module is of the target class\n        if isinstance(module, cls):\n            names = name.split('.')  # Split the module name by dots to isolate components\n            # Add the first or last part of the name (depending on the structure) to the set\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n\n    # Remove 'lm_head' if present in the set (needed for 16-bit models)\n    if 'lm_head' in lora_module_names:\n        lora_module_names.remove('lm_head')\n\n    # Return the list of linear module names\n    return list(lora_module_names)\n\n# Get the list of linear module names in the model\nmodules = find_all_linear_names(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:53:00.666387Z","iopub.execute_input":"2025-02-28T05:53:00.666756Z","iopub.status.idle":"2025-02-28T05:53:00.672727Z","shell.execute_reply.started":"2025-02-28T05:53:00.666726Z","shell.execute_reply":"2025-02-28T05:53:00.671747Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"**3. Set Up LoRA Configuration**\n\n* Configure LoRA with parameters like rank (r), scaling factor (lora_alpha), and dropout rate (lora_dropout).\n* Specify the identified linear layers as target modules for fine-tuning.","metadata":{}},{"cell_type":"code","source":"# LoRA configuration setup\npeft_config = LoraConfig(\n    r=16,                    # Rank for LoRA\n    lora_alpha=32,           # Scaling factor for LoRA\n    lora_dropout=0.05,       # Dropout rate for LoRA\n    bias=\"none\",             # No bias in LoRA layers\n    task_type=\"CAUSAL_LM\",   # Task type for causal language modeling\n    target_modules=modules   # The list of target modules (linear layers)\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:53:16.174492Z","iopub.execute_input":"2025-02-28T05:53:16.174809Z","iopub.status.idle":"2025-02-28T05:53:16.178775Z","shell.execute_reply.started":"2025-02-28T05:53:16.174786Z","shell.execute_reply":"2025-02-28T05:53:16.177921Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"**4. Prepare Tokenizer and Chat Format, and Apply LoRA to the Model**\n\n* Set the tokenizer's padding side and reset the chat template to avoid conflicts. Adapt the tokenizer and model for a conversational format using the setup_chat_format utility.\n* Integrate the LoRA configuration into the model, enabling efficient fine-tuning of the selected components.","metadata":{}},{"cell_type":"code","source":"# Set the padding side for the tokenizer (important for certain models)\ntokenizer.padding_side = 'right'\n\n# Reset chat template to ensure no leftover settings\ntokenizer.chat_template = None\n\n# Setup chat format with the model and tokenizer\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\n# Apply LoRA configurations to the model\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:53:22.934872Z","iopub.execute_input":"2025-02-28T05:53:22.935150Z","iopub.status.idle":"2025-02-28T05:53:24.421313Z","shell.execute_reply.started":"2025-02-28T05:53:22.935131Z","shell.execute_reply":"2025-02-28T05:53:24.420261Z"}},"outputs":[{"name":"stderr","text":"The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Preparing and Loading the Dataset\n\nIn this section, we load and preprocess multiple datasets related to various topics in Hindi, such as art, culture, history, cooking, health, and more. The datasets are then combined into a single unified dataset, formatted appropriately for training, and split into training and test sets.","metadata":{}},{"cell_type":"markdown","source":"**1. Loading the Datasets**\n\n* **OdiaGenAI/instruction_set_hindi_1035**: Contains instructions and responses related to art, culture, history, cooking, environment, music, and sports.\n* **SherryT997/HelpSteer-hindi**: Focuses on general question-answering conversations.\n* **kaifahmad/indian-history-hindi-QA-3.4k**: Includes questions and answers specifically about Indian history.\n* **OdiaGenAI/health_hindi_200**: Features 200 rows of health-related data.","metadata":{}},{"cell_type":"code","source":"import json\nfrom datasets import load_dataset\n\n\n# Load datasets\ninstruct_hindi = load_dataset(\"OdiaGenAI/instruction_set_hindi_1035\", split='all')\nconv_hindi = load_dataset(\"SherryT997/HelpSteer-hindi\", split='all')\nhistory_hindi = load_dataset(\"kaifahmad/indian-history-hindi-QA-3.4k\", split='all')\nhealth_hindi = load_dataset(\"OdiaGenAI/health_hindi_200\", split='all')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:53:39.744604Z","iopub.execute_input":"2025-02-28T05:53:39.744906Z","iopub.status.idle":"2025-02-28T05:53:57.264015Z","shell.execute_reply.started":"2025-02-28T05:53:39.744884Z","shell.execute_reply":"2025-02-28T05:53:57.263140Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/452 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"117de33044284b8e95caa5c6e6bc580e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"combined_instruction_hindi_1035.json:   0%|          | 0.00/1.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0b1297e6a0b4ea0ae0372707157ebb2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/1035 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff0bc11f15c842f49d34c9b75a176221"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"801afbade74d4539a39d46112865aa33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"output.jsonl:   0%|          | 0.00/9.21M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e4b1b6f402034cd1b68b3b805d778b66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/2937 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba43dfa2144f4517a6482faeac2e5293"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/1.09k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e45b92cecfb8441d94b1b005e9fff4bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/690k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d18b9e071af41dead70352e7a9abf62"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/3468 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8c5af5d4ba4047429ae3dc912896c0d6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/133 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92a47308100e4db4865e1edfc7b56cf9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"health_hindi.json:   0%|          | 0.00/151k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf013a7207d04fe9a32c2b3dd4dbad31"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/204 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"347cfa685e9c4bc4824a0aee57c24acf"}},"metadata":{}}],"execution_count":11},{"cell_type":"markdown","source":"**2 .Preprocessing the Datasets**\n\n* **Instructional and QA datasets** are reformatted into a consistent structure with \"instruction\", \"input\", and \"output\" keys.\n* **Conversation datasets** (like **HelpSteer-hindi**) are reformatted by extracting conversational turns between \"human\" and \"gpt\" from the data.","metadata":{}},{"cell_type":"code","source":"# Function to reformat datasets\ndef reformat_data(dataset, instruction_key, output_key):\n    reformatted = []\n    for entry in dataset:\n        reformatted.append({\n            \"instruction\": \"You are a helpful assistant.\",\n            \"input\": entry[instruction_key],\n            \"output\": entry[output_key]\n        })\n    return reformatted\n\n# Function to reformat conversation datasets\ndef reformat_conversation_data(dataset):\n    reformatted = []\n    for conversation in dataset['conversations']:\n        for i in range(len(conversation) - 1):  # Iterate over all conversation turns\n            if conversation[i]['from'] == 'human' and conversation[i + 1]['from'] == 'gpt':\n                reformatted.append({\n                    \"instruction\": \"You are a helpful assistant.\",\n                    \"input\": conversation[i]['value'],      # Human input\n                    \"output\": conversation[i + 1]['value']  # GPT's response\n                })\n    return reformatted\n\n# Reformat each dataset\ninstruct_data = reformat_data(instruct_hindi, \"Instruction\", \"Output\")\nhealth_data = reformat_data(health_hindi, \"Instruction\", \"Output\")\nhistory_data = reformat_data(history_hindi, \"Question\", \"Answer\")\nconversation_data = reformat_conversation_data(conv_hindi)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:53:57.265116Z","iopub.execute_input":"2025-02-28T05:53:57.265380Z","iopub.status.idle":"2025-02-28T05:53:57.503114Z","shell.execute_reply.started":"2025-02-28T05:53:57.265359Z","shell.execute_reply":"2025-02-28T05:53:57.502194Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"**3. Combining the Datasets**\n\nOnce reformatted, the datasets are combined into a single dataset that encompasses a variety of domains and question-answering tasks. This unified dataset is then saved as a JSONL file.","metadata":{}},{"cell_type":"code","source":"# Combine all datasets\ncombined_data = instruct_data + health_data + history_data + conversation_data\n\n# Write to JSONL file\noutput_file = \"hindi_dataset.jsonl\"\nwith open(output_file, \"w\", encoding=\"utf-8\") as f:\n    for entry in combined_data:\n        f.write(json.dumps(entry, ensure_ascii=False) + \"\\n\")\n\nprint(f\"Combined dataset saved to {output_file}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:53:57.504820Z","iopub.execute_input":"2025-02-28T05:53:57.505125Z","iopub.status.idle":"2025-02-28T05:53:57.621849Z","shell.execute_reply.started":"2025-02-28T05:53:57.505094Z","shell.execute_reply":"2025-02-28T05:53:57.620915Z"}},"outputs":[{"name":"stdout","text":"Combined dataset saved to hindi_dataset.jsonl\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"**4. Shuffling the Dataset**\n\nAfter loading and combining the datasets, the dataset is shuffled to introduce randomness for better generalization during training. The dataset can also be optionally truncated to a smaller subset for quicker experimentation.","metadata":{}},{"cell_type":"code","source":"# Importing the dataset and shuffling it for randomness\ndataset = load_dataset(dataset_name, split=\"all\")  # Load the entire dataset\ndataset = dataset.shuffle(seed=3117)               # Shuffle the dataset with a fixed seed for reproducibility","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:53:57.623297Z","iopub.execute_input":"2025-02-28T05:53:57.623636Z","iopub.status.idle":"2025-02-28T05:54:03.368382Z","shell.execute_reply.started":"2025-02-28T05:53:57.623607Z","shell.execute_reply":"2025-02-28T05:54:03.367741Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/972 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b079e300a0d43fa8d74f92de6a3473b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"hindi_dataset.jsonl:   0%|          | 0.00/12.0M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b81fcf468517437289f67e65f875d87d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/7644 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6e220030dc6147298d47ab5b346cb796"}},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"**5. Formatting the Dataset for Chat-based Tasks**\n\nEach row of the dataset is formatted into a chat-like structure using a custom template. The format includes a \"system\" message (instruction), a \"user\" message (input question), and an \"assistant\" message (output answer). This format is then tokenized and prepared for training.","metadata":{}},{"cell_type":"code","source":"def format_chat_template(row):\n    \"\"\"\n    This function formats each row of the dataset into a chat-like structure \n    and applies the chat template for tokenization.\n\n    Args:\n    - row: The current dataset row, containing 'instruction', 'input', and 'output'.\n\n    Returns:\n    - The updated row with a 'text' field containing the formatted chat template.\n    \"\"\"\n    # Construct a JSON-like structure for the chat conversation (system, user, assistant)\n    row_json = [\n        {\"role\": \"system\", \"content\": row[\"instruction\"]},  # System message: the instruction\n        {\"role\": \"user\", \"content\": row[\"input\"]},          # User message: the input question\n        {\"role\": \"assistant\", \"content\": row[\"output\"]}     # Assistant message: the model's response\n    ]\n    # Apply the tokenizer to format the row using the chat template without tokenizing\n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\n# Apply the chat formatting to the entire dataset using multiple processes (num_proc=4 for parallelism)\ndataset = dataset.map(format_chat_template, num_proc=4)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:54:03.369122Z","iopub.execute_input":"2025-02-28T05:54:03.369326Z","iopub.status.idle":"2025-02-28T05:54:04.934606Z","shell.execute_reply.started":"2025-02-28T05:54:03.369308Z","shell.execute_reply":"2025-02-28T05:54:04.933806Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/7644 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0362150b90c4435388a95a7b6c559e1f"}},"metadata":{}}],"execution_count":15},{"cell_type":"markdown","source":"**6. Splitting the Dataset**\n\nFinally, the dataset is split into training (90%) and test (10%) sets, which will be used for fine-tuning and evaluating the model, respectively.","metadata":{}},{"cell_type":"code","source":"# Split the dataset into training and test sets (90% train, 10% test)\ndataset = dataset.train_test_split(test_size=0.1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:54:04.935651Z","iopub.execute_input":"2025-02-28T05:54:04.935885Z","iopub.status.idle":"2025-02-28T05:54:04.953337Z","shell.execute_reply.started":"2025-02-28T05:54:04.935863Z","shell.execute_reply":"2025-02-28T05:54:04.952741Z"}},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":"# Setting Hyperparameters and Training the Model\n\nIn this section, we configure and initialize the training process using the **SFTTrainer** class, setting essential hyperparameters and training configurations for fine-tuning the model. This setup ensures an efficient and controlled training process.","metadata":{}},{"cell_type":"markdown","source":"**1. Trainer Initialization**\n\n* The **SFTTrainer** class is initialized with the model, tokenizer, training and evaluation datasets, and LoRA configuration. This class handles the entire training and evaluation workflow.\n\n**2. Hyperparameters Configuration**\n\nThe hyperparameters define how the training process will be carried out:\n\n* **Batch Size**: Both training and evaluation batches are set to 1 for memory efficiency.\n* **Gradient Accumulation**: Training steps are accumulated across two steps to simulate a larger batch size.\n* **Optimizer**: paged_adamw_32bit optimizer is used to ensure stability and efficiency.\n* **Epochs**: The model is trained for 1 epoch.\n* **Learning Rate**: A learning rate of 0.0002 is chosen to allow fine adjustments during training.\n* **Logging & Evaluation**: Training logs are saved every 10 steps, and the model is evaluated based on a set frequency.\n* **Saving Models**: The model is saved every step based on the configuration, with a maximum of two saved models.","metadata":{}},{"cell_type":"code","source":"# Setting Hyperparameters for Training\ntrainer = SFTTrainer(\n    model=model,                     # The model to be trained\n    processing_class=tokenizer,      # The tokenizer used for data processing\n    train_dataset=dataset[\"train\"],  # Training dataset\n    eval_dataset=dataset[\"test\"],    # Evaluation dataset\n    peft_config=peft_config,         # LoRA configuration for model adaptation\n    args=SFTConfig(\n        output_dir=new_model,                                   # Directory where the trained model will be saved\n        per_device_train_batch_size=1,                          # Batch size for training\n        per_device_eval_batch_size=1,                           # Batch size for evaluation\n        gradient_accumulation_steps=2,                          # Number of steps for gradient accumulation\n        optim=\"paged_adamw_32bit\",                              # Optimizer type for training\n        num_train_epochs=1,                                     # Number of training epochs\n        eval_strategy=\"steps\",                                  # Evaluation strategy during training\n        eval_steps=int(len(dataset[\"train\"]) // (1 * 2) // 5),  # Frequency of evaluation in steps\n        logging_steps=10,                                       # Frequency of logging during training\n        warmup_steps=30,                                        # Number of steps for learning rate warmup\n        logging_strategy=\"steps\",                               # Logging strategy to use (log every 'steps' steps)\n        learning_rate=0.0002,                                   # Learning rate for training\n        save_steps=0,                                           # Frequency of saving the model in steps\n        save_total_limit=0,                                     # Maximum number of saved models to keep\n        save_strategy=\"no\",                                     # Disable checkpoint saving\n        max_seq_length=512,                                     # Maximum sequence length for input data\n        fp16=True,                                              # Enable mixed precision (16-bit floating point) for training\n        bf16=False,                                             # Disable bfloat16 (use fp16 instead)\n        group_by_length=True,                                   # Group data by length for more efficient batching\n        report_to=\"none\",                                       # No external reporting (like to wandb)\n        dataset_text_field=\"text\",                              # Field name for dataset text input\n        packing=False,                                          # Disable packing of sequences for batching\n        load_best_model_at_end=False,                           # Do not load the best model after training\n        seed=3117,                                              # Set the random seed for reproducibility\n    ),\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:54:04.954074Z","iopub.execute_input":"2025-02-28T05:54:04.954379Z","iopub.status.idle":"2025-02-28T05:54:16.954172Z","shell.execute_reply.started":"2025-02-28T05:54:04.954353Z","shell.execute_reply":"2025-02-28T05:54:16.953472Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Converting train dataset to ChatML:   0%|          | 0/6879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d307d150591a48fd945cfcf2ca975449"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/6879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74c932a3bb2d418db03112e94fb396ce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/6879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c886c93ea23c4625af8fba501d21b175"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating train dataset:   0%|          | 0/6879 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc5d4f8b48344db8a331fb3da9ffa624"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Converting eval dataset to ChatML:   0%|          | 0/765 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e3fa2e51a1f4ec5a18c79b72227eb4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/765 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bbb6753cb2c4866acfd708ed672b1e2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/765 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ba88a6cfab484e12a584b69804614108"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Truncating eval dataset:   0%|          | 0/765 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37f94c7390c14a35aebed860706d23d5"}},"metadata":{}},{"name":"stderr","text":"No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"}],"execution_count":17},{"cell_type":"markdown","source":"**3. Cache Management**\n\nCaching is disabled during training to avoid excessive memory usage, ensuring smooth operation on limited hardware.","metadata":{}},{"cell_type":"code","source":"# Disable caching during training to avoid memory issues\nmodel.config.use_cache = False","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:54:16.955877Z","iopub.execute_input":"2025-02-28T05:54:16.956095Z","iopub.status.idle":"2025-02-28T05:54:16.959583Z","shell.execute_reply.started":"2025-02-28T05:54:16.956075Z","shell.execute_reply":"2025-02-28T05:54:16.958666Z"}},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":"**4. Model Training**\n\nFinally, the training process is initiated with the **train()** method, which uses the configured settings to fine-tune the model.","metadata":{}},{"cell_type":"code","source":"# Start training the model\ntrainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T05:54:16.960711Z","iopub.execute_input":"2025-02-28T05:54:16.961018Z","iopub.status.idle":"2025-02-28T07:20:06.860050Z","shell.execute_reply.started":"2025-02-28T05:54:16.960987Z","shell.execute_reply":"2025-02-28T07:20:06.859347Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3439' max='3439' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3439/3439 1:25:45, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>687</td>\n      <td>1.614200</td>\n      <td>1.708609</td>\n    </tr>\n    <tr>\n      <td>1374</td>\n      <td>1.651900</td>\n      <td>1.647516</td>\n    </tr>\n    <tr>\n      <td>2061</td>\n      <td>1.595600</td>\n      <td>1.611051</td>\n    </tr>\n    <tr>\n      <td>2748</td>\n      <td>1.365100</td>\n      <td>1.571476</td>\n    </tr>\n    <tr>\n      <td>3435</td>\n      <td>1.496400</td>\n      <td>1.552956</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=3439, training_loss=1.6087598509482084, metrics={'train_runtime': 5147.7268, 'train_samples_per_second': 1.336, 'train_steps_per_second': 0.668, 'total_flos': 1.811041434301133e+16, 'train_loss': 1.6087598509482084})"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"# Re-enable cache after training\nmodel.config.use_cache = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:20:06.860801Z","iopub.execute_input":"2025-02-28T07:20:06.861076Z","iopub.status.idle":"2025-02-28T07:20:06.864657Z","shell.execute_reply.started":"2025-02-28T07:20:06.861053Z","shell.execute_reply":"2025-02-28T07:20:06.863596Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# Saving the Fine-Tuned Adapter Model\n\nSave the fine-tuned adapter model locally for future use and deployment.","metadata":{}},{"cell_type":"code","source":"# Save the trained model to the specified directory\ntrainer.model.save_pretrained(new_model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:21:39.065454Z","iopub.execute_input":"2025-02-28T07:21:39.065934Z","iopub.status.idle":"2025-02-28T07:21:45.474890Z","shell.execute_reply.started":"2025-02-28T07:21:39.065895Z","shell.execute_reply":"2025-02-28T07:21:45.474216Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":21},{"cell_type":"markdown","source":"# Inference and Generating Responses\n\nThis section focuses on loading the fine-tuned model, configuring it for inference, and generating responses to user queries. The workflow involves preparing the model and tokenizer, formatting the input, and decoding the model's output to generate meaningful answers.","metadata":{}},{"cell_type":"markdown","source":"**1. Clear CUDA Cache**\n\nBefore inference, the CUDA memory cache is cleared to optimize GPU memory usage and prevent memory-related issues.","metadata":{}},{"cell_type":"code","source":"# Clear the CUDA memory cache.\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:22:00.426786Z","iopub.execute_input":"2025-02-28T07:22:00.427070Z","iopub.status.idle":"2025-02-28T07:22:00.673353Z","shell.execute_reply.started":"2025-02-28T07:22:00.427049Z","shell.execute_reply":"2025-02-28T07:22:00.672399Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"**2. Set Up the Model for Inference**\n\nLoad the fine-tuned model with 4-bit quantization and integrate it with the base model:\n* **Quantization Configuration**: Applies 4-bit quantization to optimize memory and computational efficiency.\n* **Model Loading**: Loads the base model and fine-tuned weights, setting it to evaluation mode for inference.","metadata":{}},{"cell_type":"code","source":"# Define the path to the fine-tuned model\nnew_model_path = \"/kaggle/working/Gemma-2-2b-it-hindi\"\n\n# Configuration for 4-bit quantization to optimize model performance and memory usage\nquantization_config = BitsAndBytesConfig(\n    load_in_4bit=True,                     # Enable 4-bit quantization for efficient loading\n    bnb_4bit_quant_type=\"nf4\",             # Use NormalFloat4 (NF4) quantization type for better accuracy\n    bnb_4bit_compute_dtype=torch.float16,  # Use 16-bit floating-point precision for computations\n    bnb_4bit_use_double_quant=True         # Enable double quantization for improved numerical stability\n)\n\n# Load the base model with QLoRA (Quantized LoRA) configuration\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,                              # Path or identifier of the base model\n    quantization_config=quantization_config, # Apply the quantization configuration\n    attn_implementation=\"eager\",             # Set attention mechanism implementation to \"eager\"\n    torch_dtype=torch.float16,               # Use 16-bit floating-point precision for weights and activations\n    return_dict=True,                        # Return outputs as a dictionary for better readability\n    device_map=\"auto\"                        # Automatically map model components to available devices\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:22:32.123782Z","iopub.execute_input":"2025-02-28T07:22:32.124201Z","iopub.status.idle":"2025-02-28T07:22:38.776556Z","shell.execute_reply.started":"2025-02-28T07:22:32.124162Z","shell.execute_reply":"2025-02-28T07:22:38.775896Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d85da324192c41d99cbac6d75c6695a6"}},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"**3. Prepare the Tokenizer**\n\nThe tokenizer is initialized to process inputs and generate outputs in a chat format. Any previous configurations are reset to avoid interference with new tasks.","metadata":{}},{"cell_type":"code","source":"# Load the tokenizer for the base model\ntokenizer = AutoTokenizer.from_pretrained(base_model)\n\n# Reset the chat template to ensure no stale settings interfere with new tasks\ntokenizer.chat_template = None\n\n# Configure the model and tokenizer for chat-based interactions\nmodel, tokenizer = setup_chat_format(model, tokenizer)\n\n# Load the fine-tuned model with PeftModel, applying it to the base model\nmodel = PeftModel.from_pretrained(model, new_model_path)\n\n# Set the model to evaluation mode to prepare for inference\nmodel.eval()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:23:33.056033Z","iopub.execute_input":"2025-02-28T07:23:33.056312Z","iopub.status.idle":"2025-02-28T07:23:35.865089Z","shell.execute_reply.started":"2025-02-28T07:23:33.056291Z","shell.execute_reply":"2025-02-28T07:23:35.864343Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma2ForCausalLM(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256002, 2304, padding_idx=0)\n        (layers): ModuleList(\n          (0-25): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=9216, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (rotary_emb): Gemma2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2304, out_features=256002, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:24:23.336667Z","iopub.execute_input":"2025-02-28T07:24:23.336978Z","iopub.status.idle":"2025-02-28T07:24:23.351065Z","shell.execute_reply.started":"2025-02-28T07:24:23.336952Z","shell.execute_reply":"2025-02-28T07:24:23.350307Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"PeftModelForCausalLM(\n  (base_model): LoraModel(\n    (model): Gemma2ForCausalLM(\n      (model): Gemma2Model(\n        (embed_tokens): Embedding(256002, 2304, padding_idx=0)\n        (layers): ModuleList(\n          (0-25): 26 x Gemma2DecoderLayer(\n            (self_attn): Gemma2Attention(\n              (q_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=2048, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2048, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (k_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (v_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=1024, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=1024, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (o_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2048, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2048, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n            )\n            (mlp): Gemma2MLP(\n              (gate_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (up_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=2304, out_features=9216, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=2304, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=9216, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (down_proj): lora.Linear4bit(\n                (base_layer): Linear4bit(in_features=9216, out_features=2304, bias=False)\n                (lora_dropout): ModuleDict(\n                  (default): Dropout(p=0.05, inplace=False)\n                )\n                (lora_A): ModuleDict(\n                  (default): Linear(in_features=9216, out_features=16, bias=False)\n                )\n                (lora_B): ModuleDict(\n                  (default): Linear(in_features=16, out_features=2304, bias=False)\n                )\n                (lora_embedding_A): ParameterDict()\n                (lora_embedding_B): ParameterDict()\n                (lora_magnitude_vector): ModuleDict()\n              )\n              (act_fn): PytorchGELUTanh()\n            )\n            (input_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_attention_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (pre_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n            (post_feedforward_layernorm): Gemma2RMSNorm((2304,), eps=1e-06)\n          )\n        )\n        (norm): Gemma2RMSNorm((2304,), eps=1e-06)\n        (rotary_emb): Gemma2RotaryEmbedding()\n      )\n      (lm_head): Linear(in_features=2304, out_features=256002, bias=False)\n    )\n  )\n)"},"metadata":{}}],"execution_count":26},{"cell_type":"markdown","source":"**4. Define the Conversation and Create a Prompt**\n\nFormat the conversation history into a structured prompt using the tokenizers chat template. This ensures the model receives well-structured input.","metadata":{}},{"cell_type":"code","source":"# Define the conversation history as a list of messages\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n    {\"role\": \"user\", \"content\": \"      ?\"},\n]\n\n# Apply the tokenizer's chat template to format the messages for the model\n# Set tokenize=False to avoid tokenization at this point, and add_generation_prompt=True to prepare for generation\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# Tokenize the prompt and prepare the inputs for the model\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:27:11.561072Z","iopub.execute_input":"2025-02-28T07:27:11.561470Z","iopub.status.idle":"2025-02-28T07:27:11.568062Z","shell.execute_reply.started":"2025-02-28T07:27:11.561438Z","shell.execute_reply":"2025-02-28T07:27:11.567253Z"}},"outputs":[],"execution_count":30},{"cell_type":"markdown","source":"**5. Generate a Response**\n\nUse the model to generate a response, applying sampling techniques for diverse and high-quality outputs:\n\n* **Max Length**: Limits the response length to 256 tokens.\n* **Top-K Sampling**: Considers the top 50 tokens at each step.\n* **Nucleus Sampling (Top-P)**: Ensures 85% cumulative probability, balancing diversity and relevance.\n* **Temperature**: A low value (0.3) makes the output more deterministic.\n* **No Repetition**: Prevents repetitive phrases by disallowing 3-gram repetitions.","metadata":{}},{"cell_type":"code","source":"# Optimized text generation with custom sampling strategies for better results\noutputs = model.generate(\n    **inputs,                # Feed the tokenized inputs to the model\n    max_length=256,          # Limit the maximum length of the generated text (512 tokens)\n    num_return_sequences=1,  # Only return one sequence of text\n    top_k=50,                # Limit the sampling pool to the top 50 tokens\n    top_p=0.85,              # Use nucleus sampling with a cumulative probability of 85% (more deterministic output)\n    temperature=0.3,         # Lower temperature for more deterministic (less random) responses\n    no_repeat_ngram_size=3,  # Prevent repeating n-grams of size 3 (e.g., \"the the the\")\n    do_sample=True,          # Enable sampling for more diverse outputs (as opposed to greedy decoding)\n    num_beams=20             # This parameter controls the number of beams used during beam search.\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:30:12.134872Z","iopub.execute_input":"2025-02-28T07:30:12.135184Z","iopub.status.idle":"2025-02-28T07:30:54.141233Z","shell.execute_reply.started":"2025-02-28T07:30:12.135156Z","shell.execute_reply":"2025-02-28T07:30:54.140567Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"**6. Decode and Extract the Response**\n\nDecode the generated output into human-readable text, cleaning unnecessary parts to extract the final response.","metadata":{}},{"cell_type":"code","source":"# Decode the output sequence back to text, skipping special tokens like padding and EOS markers\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract the assistant's response from the generated text (split at \"assistant\" to clean up)\nresponse = text.split(\"assistant\")[2].strip()  # Remove unwanted parts and get the final response\n\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-28T07:30:54.142324Z","iopub.execute_input":"2025-02-28T07:30:54.142604Z","iopub.status.idle":"2025-02-28T07:30:54.148124Z","shell.execute_reply.started":"2025-02-28T07:30:54.142569Z","shell.execute_reply":"2025-02-28T07:30:54.147288Z"}},"outputs":[{"name":"stdout","text":"          -             :\n\n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n- \n\n                ,                 , \"\"                  ,  \"\"             , \"\", \"\"  \"\" \n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}